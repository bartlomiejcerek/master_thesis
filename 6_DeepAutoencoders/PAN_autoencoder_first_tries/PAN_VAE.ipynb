{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PAN_VAE.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"LCQBk-cXMR1R","colab_type":"text"},"source":["# Simple autoencoder\n","\n","## 1.Import modules and set paths\n","Modify here to load files from different places.\n","\n","All used libraries are here."]},{"cell_type":"code","metadata":{"id":"c4hXmiQRMEBt","colab_type":"code","outputId":"e3190250-7c87-49fd-f740-d02e70bf0462","executionInfo":{"status":"ok","timestamp":1559067433657,"user_tz":-120,"elapsed":654,"user":{"displayName":"Bartłomiej Cerek","photoUrl":"https://lh6.googleusercontent.com/-63SSrSJFeXQ/AAAAAAAAAAI/AAAAAAAAF5A/_OvOPODWwwg/s64/photo.jpg","userId":"03865172822680659937"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# Mount gdrive\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","# Data managment \n","import pandas as pd \n","import numpy as np \n","from sklearn.preprocessing import MaxAbsScaler\n","\n","import torch as t\n","from torch import nn, optim\n","from torch.utils import data as data_lib\n","import torch.nn.functional as F\n","\n","proj_folder = '/content/gdrive/My Drive/Colab Notebooks/PAN_autoencoder/'\n","data_folder = proj_folder + 'data/'\n","solutions_folder = proj_folder + 'data/solutions/'"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"P3IDH-swNjml","colab_type":"text"},"source":["## 2.Load and select data\n","In this model **alias_global_Outlier** will be used as label."]},{"cell_type":"code","metadata":{"id":"PKubEOzzOYzr","colab_type":"code","outputId":"63bee775-f80a-4b9e-b64d-d482dc662f35","executionInfo":{"status":"ok","timestamp":1559067440235,"user_tz":-120,"elapsed":7213,"user":{"displayName":"Bartłomiej Cerek","photoUrl":"https://lh6.googleusercontent.com/-63SSrSJFeXQ/AAAAAAAAAAI/AAAAAAAAF5A/_OvOPODWwwg/s64/photo.jpg","userId":"03865172822680659937"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["#Load data, separatly, it takes long time.\n","raw = pd.read_csv(data_folder + 'trending_merged_LHC18q_withGraphs.csv')\n","#Check shape\n","print(raw.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(2508, 7226)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"j-GsEzNdNmjy","colab_type":"code","outputId":"d32e9683-814a-419e-b514-e31f07fa420d","executionInfo":{"status":"ok","timestamp":1559067440236,"user_tz":-120,"elapsed":7198,"user":{"displayName":"Bartłomiej Cerek","photoUrl":"https://lh6.googleusercontent.com/-63SSrSJFeXQ/AAAAAAAAAAI/AAAAAAAAF5A/_OvOPODWwwg/s64/photo.jpg","userId":"03865172822680659937"}},"colab":{"base_uri":"https://localhost:8080/","height":269}},"source":["#Define meta data, graphs, aliases\n","meta_cols = ['run', 'chunkID', 'time', 'year', 'period.fString', 'pass.fString', \n","'runType.fString', 'startTimeGRP', 'stopTimeGRP', 'duration', 'chunkStart', \n","'chunkStop', 'chunkMean', 'chunkMedian', 'chunkRMS', \n","'chunkDuration'] + ['Unnamed: 0', 'dataType.fString' ] # After + my opinion\n","\n","graph_cols = [col for col in raw if col.startswith('gr')] # +6k\n","\n","alias_cols = [col for col in raw if col.startswith('alias_')] #265\n","\n","#Point to label columns\n","war_col = 'alias_global_Warning'\n","out_col = 'alias_global_Outlier'\n","\n","# Filter out data not used for classification.\n","filtered = raw.drop(meta_cols + graph_cols + alias_cols, axis=1)\n","no_samples, no_features = filtered.shape\n","\n","#Check shape\n","print(no_samples, no_features)\n","filtered.head()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["2508 239\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>bz</th>\n","      <th>meanTPCnclF</th>\n","      <th>rmsTPCnclF</th>\n","      <th>meanTPCChi2</th>\n","      <th>rmsTPCChi2</th>\n","      <th>slopeATPCnclF</th>\n","      <th>slopeCTPCnclF</th>\n","      <th>slopeATPCnclFErr</th>\n","      <th>slopeCTPCnclFErr</th>\n","      <th>meanTPCncl</th>\n","      <th>rmsTPCncl</th>\n","      <th>slopeATPCncl</th>\n","      <th>slopeCTPCncl</th>\n","      <th>slopeATPCnclErr</th>\n","      <th>slopeCTPCnclErr</th>\n","      <th>hasRawQA</th>\n","      <th>rawClusterCounter</th>\n","      <th>rawSignalCounter</th>\n","      <th>offsetdZA</th>\n","      <th>slopedZA</th>\n","      <th>offsetdZC</th>\n","      <th>slopedZC</th>\n","      <th>offsetdZAErr</th>\n","      <th>slopedZAErr</th>\n","      <th>offsetdZCErr</th>\n","      <th>slopedZCErr</th>\n","      <th>offsetdZAchi2</th>\n","      <th>slopedZAchi2</th>\n","      <th>offsetdZCchi2</th>\n","      <th>slopedZCchi2</th>\n","      <th>offsetdZAPos</th>\n","      <th>slopedZAPos</th>\n","      <th>offsetdZCPos</th>\n","      <th>slopedZCPos</th>\n","      <th>offsetdZAErrPos</th>\n","      <th>slopedZAErrPos</th>\n","      <th>offsetdZCErrPos</th>\n","      <th>slopedZCErrPos</th>\n","      <th>offsetdZAchi2Pos</th>\n","      <th>slopedZAchi2Pos</th>\n","      <th>...</th>\n","      <th>oroc_C_side</th>\n","      <th>MIPattachSlopeC</th>\n","      <th>MIPattachSlopeA</th>\n","      <th>resolutionMIP</th>\n","      <th>meanMIP</th>\n","      <th>meanMIPele</th>\n","      <th>resolutionMIPele</th>\n","      <th>electroMIPSeparation</th>\n","      <th>tpcItsMatchA</th>\n","      <th>tpcItsMatchHighPtA</th>\n","      <th>tpcItsMatchC</th>\n","      <th>tpcItsMatchHighPtC</th>\n","      <th>phiPull</th>\n","      <th>phiPullHighPt</th>\n","      <th>ptPull</th>\n","      <th>ptPullHighPt</th>\n","      <th>yPull</th>\n","      <th>yPullHighPt</th>\n","      <th>zPull</th>\n","      <th>zPullHighPt</th>\n","      <th>lambdaPull</th>\n","      <th>lambdaPullHighPt</th>\n","      <th>tpcConstrainPhiA</th>\n","      <th>tpcConstrainPhiC</th>\n","      <th>meanPTRelativeA</th>\n","      <th>medianPTRelativeA</th>\n","      <th>rmsPTRelativeA</th>\n","      <th>meanPTRelativeC</th>\n","      <th>medianPTRelativeC</th>\n","      <th>rmsPTRelativeC</th>\n","      <th>meanHVandPTGainCorrIROC</th>\n","      <th>medianHVandPTGainCorrIROC</th>\n","      <th>rmsHVandPTGainCorrIROC</th>\n","      <th>meanHVandPTGainCorrOROC</th>\n","      <th>medianHVandPTGainCorrOROC</th>\n","      <th>rmsHVandPTGainCorrOROC</th>\n","      <th>meanVDriftCorr</th>\n","      <th>medianVDriftCorr</th>\n","      <th>rmsVDriftCorr</th>\n","      <th>interactionRate</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.5</td>\n","      <td>0.848773</td>\n","      <td>0.091146</td>\n","      <td>2.074478</td>\n","      <td>0.542143</td>\n","      <td>0.042352</td>\n","      <td>-0.043842</td>\n","      <td>0.000288</td>\n","      <td>-0.043842</td>\n","      <td>124.531530</td>\n","      <td>20.885829</td>\n","      <td>7.019171</td>\n","      <td>-6.920046</td>\n","      <td>0.066220</td>\n","      <td>-6.920046</td>\n","      <td>1</td>\n","      <td>306440923</td>\n","      <td>-1079234484</td>\n","      <td>0.022998</td>\n","      <td>0.067565</td>\n","      <td>0.105553</td>\n","      <td>0.341043</td>\n","      <td>0.000650</td>\n","      <td>0.001455</td>\n","      <td>0.000702</td>\n","      <td>0.001524</td>\n","      <td>459.183559</td>\n","      <td>459.183559</td>\n","      <td>16289.403769</td>\n","      <td>16289.403769</td>\n","      <td>0.026751</td>\n","      <td>0.078602</td>\n","      <td>0.099011</td>\n","      <td>0.347376</td>\n","      <td>0.000921</td>\n","      <td>0.002062</td>\n","      <td>0.000997</td>\n","      <td>0.002162</td>\n","      <td>208.497740</td>\n","      <td>208.497740</td>\n","      <td>...</td>\n","      <td>18</td>\n","      <td>-1.312186</td>\n","      <td>0.901106</td>\n","      <td>0.071071</td>\n","      <td>49.080948</td>\n","      <td>77.109420</td>\n","      <td>0.058643</td>\n","      <td>28.028473</td>\n","      <td>0.821641</td>\n","      <td>0.707246</td>\n","      <td>0.827993</td>\n","      <td>0.718884</td>\n","      <td>0.001086</td>\n","      <td>-0.004639</td>\n","      <td>0.611592</td>\n","      <td>0.559278</td>\n","      <td>-0.066643</td>\n","      <td>-0.227419</td>\n","      <td>-0.097121</td>\n","      <td>-0.085052</td>\n","      <td>0.156918</td>\n","      <td>0.084211</td>\n","      <td>0.022553</td>\n","      <td>0.028088</td>\n","      <td>0.003410</td>\n","      <td>0.003406</td>\n","      <td>0.000016</td>\n","      <td>0.003479</td>\n","      <td>0.003475</td>\n","      <td>0.000016</td>\n","      <td>1.128303</td>\n","      <td>1.128288</td>\n","      <td>0.000061</td>\n","      <td>1.092745</td>\n","      <td>1.092730</td>\n","      <td>0.000059</td>\n","      <td>-0.024503</td>\n","      <td>-0.024503</td>\n","      <td>7.113417e-07</td>\n","      <td>713.363095</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.5</td>\n","      <td>0.847625</td>\n","      <td>0.090930</td>\n","      <td>2.078402</td>\n","      <td>0.540485</td>\n","      <td>0.043480</td>\n","      <td>-0.045471</td>\n","      <td>0.000280</td>\n","      <td>-0.045471</td>\n","      <td>124.367730</td>\n","      <td>20.829782</td>\n","      <td>7.229518</td>\n","      <td>-7.157330</td>\n","      <td>0.064418</td>\n","      <td>-7.157330</td>\n","      <td>1</td>\n","      <td>306440923</td>\n","      <td>-1079234484</td>\n","      <td>0.025370</td>\n","      <td>0.062760</td>\n","      <td>0.095961</td>\n","      <td>0.323425</td>\n","      <td>0.000634</td>\n","      <td>0.001418</td>\n","      <td>0.000679</td>\n","      <td>0.001479</td>\n","      <td>453.342481</td>\n","      <td>453.342481</td>\n","      <td>16309.115444</td>\n","      <td>16309.115444</td>\n","      <td>0.030112</td>\n","      <td>0.071605</td>\n","      <td>0.091458</td>\n","      <td>0.333769</td>\n","      <td>0.000900</td>\n","      <td>0.002011</td>\n","      <td>0.000964</td>\n","      <td>0.002096</td>\n","      <td>169.984488</td>\n","      <td>169.984488</td>\n","      <td>...</td>\n","      <td>18</td>\n","      <td>-1.324162</td>\n","      <td>0.926544</td>\n","      <td>0.071175</td>\n","      <td>49.072520</td>\n","      <td>77.138400</td>\n","      <td>0.058660</td>\n","      <td>28.065876</td>\n","      <td>0.823531</td>\n","      <td>0.712409</td>\n","      <td>0.831177</td>\n","      <td>0.713895</td>\n","      <td>0.000842</td>\n","      <td>0.003125</td>\n","      <td>0.627048</td>\n","      <td>0.496875</td>\n","      <td>-0.069809</td>\n","      <td>-0.019355</td>\n","      <td>-0.111800</td>\n","      <td>-0.182812</td>\n","      <td>0.174105</td>\n","      <td>0.045833</td>\n","      <td>0.023366</td>\n","      <td>0.027590</td>\n","      <td>0.003472</td>\n","      <td>0.003473</td>\n","      <td>0.000016</td>\n","      <td>0.003540</td>\n","      <td>0.003542</td>\n","      <td>0.000016</td>\n","      <td>1.128544</td>\n","      <td>1.128551</td>\n","      <td>0.000065</td>\n","      <td>1.092978</td>\n","      <td>1.092985</td>\n","      <td>0.000063</td>\n","      <td>-0.024506</td>\n","      <td>-0.024506</td>\n","      <td>5.334679e-07</td>\n","      <td>713.363095</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.5</td>\n","      <td>0.848872</td>\n","      <td>0.091416</td>\n","      <td>2.071204</td>\n","      <td>0.541242</td>\n","      <td>0.042393</td>\n","      <td>-0.043826</td>\n","      <td>0.000287</td>\n","      <td>-0.043826</td>\n","      <td>124.549014</td>\n","      <td>20.905742</td>\n","      <td>7.113535</td>\n","      <td>-6.839295</td>\n","      <td>0.065815</td>\n","      <td>-6.839295</td>\n","      <td>1</td>\n","      <td>306440923</td>\n","      <td>-1079234484</td>\n","      <td>0.025024</td>\n","      <td>0.061236</td>\n","      <td>0.088003</td>\n","      <td>0.313389</td>\n","      <td>0.000643</td>\n","      <td>0.001439</td>\n","      <td>0.000691</td>\n","      <td>0.001504</td>\n","      <td>453.240478</td>\n","      <td>453.240478</td>\n","      <td>15354.985294</td>\n","      <td>15354.985294</td>\n","      <td>0.031010</td>\n","      <td>0.066596</td>\n","      <td>0.082599</td>\n","      <td>0.323772</td>\n","      <td>0.000911</td>\n","      <td>0.002036</td>\n","      <td>0.000982</td>\n","      <td>0.002137</td>\n","      <td>167.321561</td>\n","      <td>167.321561</td>\n","      <td>...</td>\n","      <td>18</td>\n","      <td>-1.310997</td>\n","      <td>0.922359</td>\n","      <td>0.071010</td>\n","      <td>49.108963</td>\n","      <td>77.188070</td>\n","      <td>0.058650</td>\n","      <td>28.079110</td>\n","      <td>0.822266</td>\n","      <td>0.702680</td>\n","      <td>0.828755</td>\n","      <td>0.718356</td>\n","      <td>-0.001249</td>\n","      <td>0.002857</td>\n","      <td>0.607328</td>\n","      <td>0.322857</td>\n","      <td>-0.049271</td>\n","      <td>0.068750</td>\n","      <td>-0.100709</td>\n","      <td>-0.340000</td>\n","      <td>0.157151</td>\n","      <td>-0.050000</td>\n","      <td>0.021696</td>\n","      <td>0.027795</td>\n","      <td>0.003472</td>\n","      <td>0.003473</td>\n","      <td>0.000016</td>\n","      <td>0.003540</td>\n","      <td>0.003542</td>\n","      <td>0.000016</td>\n","      <td>1.128544</td>\n","      <td>1.128551</td>\n","      <td>0.000065</td>\n","      <td>1.092978</td>\n","      <td>1.092985</td>\n","      <td>0.000063</td>\n","      <td>-0.024506</td>\n","      <td>-0.024506</td>\n","      <td>5.334679e-07</td>\n","      <td>713.363095</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.5</td>\n","      <td>0.848067</td>\n","      <td>0.091023</td>\n","      <td>2.073758</td>\n","      <td>0.540435</td>\n","      <td>0.043119</td>\n","      <td>-0.044706</td>\n","      <td>0.000286</td>\n","      <td>-0.044706</td>\n","      <td>124.428344</td>\n","      <td>20.855069</td>\n","      <td>7.284781</td>\n","      <td>-7.006315</td>\n","      <td>0.065994</td>\n","      <td>-7.006315</td>\n","      <td>1</td>\n","      <td>306440923</td>\n","      <td>-1079234484</td>\n","      <td>0.018161</td>\n","      <td>0.066117</td>\n","      <td>0.082855</td>\n","      <td>0.296620</td>\n","      <td>0.000649</td>\n","      <td>0.001450</td>\n","      <td>0.000689</td>\n","      <td>0.001501</td>\n","      <td>450.800398</td>\n","      <td>450.800398</td>\n","      <td>14733.873233</td>\n","      <td>14733.873233</td>\n","      <td>0.024858</td>\n","      <td>0.069303</td>\n","      <td>0.076956</td>\n","      <td>0.303873</td>\n","      <td>0.000921</td>\n","      <td>0.002055</td>\n","      <td>0.000975</td>\n","      <td>0.002125</td>\n","      <td>146.630890</td>\n","      <td>146.630890</td>\n","      <td>...</td>\n","      <td>18</td>\n","      <td>-1.319607</td>\n","      <td>0.917074</td>\n","      <td>0.071124</td>\n","      <td>49.132610</td>\n","      <td>77.231010</td>\n","      <td>0.058535</td>\n","      <td>28.098400</td>\n","      <td>0.824839</td>\n","      <td>0.708403</td>\n","      <td>0.827347</td>\n","      <td>0.718256</td>\n","      <td>-0.000301</td>\n","      <td>0.010294</td>\n","      <td>0.655326</td>\n","      <td>0.401471</td>\n","      <td>-0.118862</td>\n","      <td>-0.351538</td>\n","      <td>-0.081881</td>\n","      <td>-0.208824</td>\n","      <td>0.158344</td>\n","      <td>0.144828</td>\n","      <td>0.023284</td>\n","      <td>0.027317</td>\n","      <td>0.003503</td>\n","      <td>0.003504</td>\n","      <td>0.000003</td>\n","      <td>0.003572</td>\n","      <td>0.003573</td>\n","      <td>0.000003</td>\n","      <td>1.128695</td>\n","      <td>1.128677</td>\n","      <td>0.000086</td>\n","      <td>1.093101</td>\n","      <td>1.093104</td>\n","      <td>0.000011</td>\n","      <td>-0.024507</td>\n","      <td>-0.024507</td>\n","      <td>7.156574e-08</td>\n","      <td>713.363095</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.5</td>\n","      <td>0.848658</td>\n","      <td>0.090803</td>\n","      <td>2.070193</td>\n","      <td>0.538202</td>\n","      <td>0.042675</td>\n","      <td>-0.044115</td>\n","      <td>0.000282</td>\n","      <td>-0.044115</td>\n","      <td>124.530243</td>\n","      <td>20.833240</td>\n","      <td>7.079192</td>\n","      <td>-6.986591</td>\n","      <td>0.065170</td>\n","      <td>-6.986591</td>\n","      <td>1</td>\n","      <td>306440923</td>\n","      <td>-1079234484</td>\n","      <td>0.012684</td>\n","      <td>0.069024</td>\n","      <td>0.075969</td>\n","      <td>0.283900</td>\n","      <td>0.000642</td>\n","      <td>0.001437</td>\n","      <td>0.000684</td>\n","      <td>0.001498</td>\n","      <td>394.740895</td>\n","      <td>394.740895</td>\n","      <td>13542.270975</td>\n","      <td>13542.270975</td>\n","      <td>0.017779</td>\n","      <td>0.076171</td>\n","      <td>0.070564</td>\n","      <td>0.292695</td>\n","      <td>0.000913</td>\n","      <td>0.002041</td>\n","      <td>0.000971</td>\n","      <td>0.002124</td>\n","      <td>116.179449</td>\n","      <td>116.179449</td>\n","      <td>...</td>\n","      <td>18</td>\n","      <td>-1.318750</td>\n","      <td>0.920233</td>\n","      <td>0.070995</td>\n","      <td>49.170532</td>\n","      <td>77.265976</td>\n","      <td>0.058852</td>\n","      <td>28.095444</td>\n","      <td>0.821044</td>\n","      <td>0.691395</td>\n","      <td>0.828724</td>\n","      <td>0.713096</td>\n","      <td>-0.000770</td>\n","      <td>-0.002174</td>\n","      <td>0.639943</td>\n","      <td>0.444203</td>\n","      <td>-0.017350</td>\n","      <td>-0.318750</td>\n","      <td>-0.109731</td>\n","      <td>-0.145652</td>\n","      <td>0.088306</td>\n","      <td>-0.122727</td>\n","      <td>0.022234</td>\n","      <td>0.027934</td>\n","      <td>0.003487</td>\n","      <td>0.003487</td>\n","      <td>0.000010</td>\n","      <td>0.003556</td>\n","      <td>0.003556</td>\n","      <td>0.000010</td>\n","      <td>1.128609</td>\n","      <td>1.128612</td>\n","      <td>0.000038</td>\n","      <td>1.093041</td>\n","      <td>1.093044</td>\n","      <td>0.000037</td>\n","      <td>-0.024506</td>\n","      <td>-0.024506</td>\n","      <td>5.608254e-07</td>\n","      <td>713.363095</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 239 columns</p>\n","</div>"],"text/plain":["    bz  meanTPCnclF  ...  rmsVDriftCorr  interactionRate\n","0  0.5     0.848773  ...   7.113417e-07       713.363095\n","1  0.5     0.847625  ...   5.334679e-07       713.363095\n","2  0.5     0.848872  ...   5.334679e-07       713.363095\n","3  0.5     0.848067  ...   7.156574e-08       713.363095\n","4  0.5     0.848658  ...   5.608254e-07       713.363095\n","\n","[5 rows x 239 columns]"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"I_QycZXOi0tS","colab_type":"text"},"source":["## 3.Create and shuffle indices\n","Also check amount of flagged and not flagged"]},{"cell_type":"code","metadata":{"id":"YsVUyHx0Y-V4","colab_type":"code","outputId":"cc4a4161-803f-40b5-df0f-a9edb25721cf","executionInfo":{"status":"ok","timestamp":1559067440238,"user_tz":-120,"elapsed":7185,"user":{"displayName":"Bartłomiej Cerek","photoUrl":"https://lh6.googleusercontent.com/-63SSrSJFeXQ/AAAAAAAAAAI/AAAAAAAAF5A/_OvOPODWwwg/s64/photo.jpg","userId":"03865172822680659937"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["# Always there is warning when there is error!\n","\n","# [0] becasue silly 'where' returns touple\n","crit = raw[out_col].astype(bool)\n","f_indices = np.where(crit)[0] # Flagged indices\n","no_f_indices = np.where(~crit)[0] # Not flagged indices\n","\n","# Number of flagged elements\n","print(len(f_indices))\n","# Number of not flagged elements\n","print(len(no_f_indices))\n","\n","#Shuffle intances to ensure that with each run different samples are drawn\n","random_seed= 42 # Controlled randomnes\n","np.random.seed(random_seed)\n","np.random.shuffle(f_indices)\n","np.random.shuffle(no_f_indices)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["71\n","2437\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-01U0zg-aIJ0","colab_type":"text"},"source":["## 4.Preprocess and create generators.\n","Generators will be used for training, validation and test.\n","\n","Some specific features of PyTorch are used."]},{"cell_type":"code","metadata":{"id":"EvBT55eubIDk","colab_type":"code","outputId":"47d6356e-bba1-4f9d-8b70-f140b86967ec","executionInfo":{"status":"ok","timestamp":1559067440922,"user_tz":-120,"elapsed":7859,"user":{"displayName":"Bartłomiej Cerek","photoUrl":"https://lh6.googleusercontent.com/-63SSrSJFeXQ/AAAAAAAAAAI/AAAAAAAAF5A/_OvOPODWwwg/s64/photo.jpg","userId":"03865172822680659937"}},"colab":{"base_uri":"https://localhost:8080/","height":156}},"source":["#Parameters\n","params = {'batch_size': 10,\n","          'num_workers': 8}\n","scaler = MaxAbsScaler()\n","\n","# 10% of all correct instances will be used as validation\n","valid_split = int(np.floor(.1 * len(no_f_indices))) \n","# 70 correct and 70 flagged samples will be used as test\n","test_split = 70\n","\n","#Create dataset\n","X = t.FloatTensor(scaler.fit_transform(filtered)) #features\n","Y = t.IntTensor(raw[out_col]) #labels\n","dataset = data_lib.TensorDataset(X,Y)\n","\n","#Set training indices of correct cases\n","test_indices_c, valid_indices, train_indices = np.split(no_f_indices, [test_split, test_split+valid_split])\n","# Add flagged indices to test indices \n","test_indices = np.concatenate((test_indices_c, f_indices[:test_split]))\n","\n","# Create samplers\n","train_sampler = data_lib.SubsetRandomSampler(train_indices)\n","valid_sampler = data_lib.SubsetRandomSampler(valid_indices)\n","test_sampler = data_lib.SubsetRandomSampler(test_indices)\n","\n","#Create generators\n","train_gen = data_lib.DataLoader(dataset, **params,sampler=train_sampler)\n","valid_gen = data_lib.DataLoader(dataset, **params, sampler=valid_sampler)\n","test_gen = data_lib.DataLoader(dataset, **params, sampler=test_sampler)\n","\n","#Check if all fine\n","next(iter(test_gen))"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[tensor([[ 1.0000,  0.9452,  0.8640,  ..., -0.6960,  0.2413,  0.2316],\n","         [ 1.0000,  0.9313,  0.8812,  ..., -0.9346,  0.0422,  0.9601],\n","         [ 1.0000,  0.9434,  0.8664,  ..., -0.6993,  0.1885,  0.3014],\n","         ...,\n","         [ 1.0000,  0.9505,  0.8505,  ..., -0.7307,  0.0870,  0.0700],\n","         [ 1.0000,  0.9301,  0.8872,  ..., -0.7424,  0.0065,  0.9700],\n","         [ 1.0000,  0.9309,  0.8894,  ..., -0.6851,  0.0991,  0.8945]]),\n"," tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 0], dtype=torch.int32)]"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"1YstusgBr8Ss","colab_type":"text"},"source":["## 5.Prepare for training\n","\n","Check if training on GPU is possible and if yes move model.\n","GPU variable needs to be establish before model becasue there are other cuda calculations in forward() than just linears. "]},{"cell_type":"code","metadata":{"id":"q13i7kCqVZ_g","colab_type":"code","outputId":"d317b88a-b7b5-4cff-bce4-b746228c21a9","executionInfo":{"status":"ok","timestamp":1559067440925,"user_tz":-120,"elapsed":7846,"user":{"displayName":"Bartłomiej Cerek","photoUrl":"https://lh6.googleusercontent.com/-63SSrSJFeXQ/AAAAAAAAAAI/AAAAAAAAF5A/_OvOPODWwwg/s64/photo.jpg","userId":"03865172822680659937"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# check if CUDA is available\n","gpu = t.cuda.is_available()\n","\n","if not gpu:\n","    print('CUDA is not available.  Training on CPU ...')\n","\n","else:\n","    print('CUDA is available!  Training on GPU ...')\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["CUDA is available!  Training on GPU ...\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"uB0TxZ2kueYX","colab_type":"text"},"source":["## 6.Defining architecture of Neural Network\n","Variational autoencoder uses probabilities distributions among latent variables. \n","\n","It requires defining more complex architecture and loss function but usually gives better results. \n","\n","It has extra feature allowing for generating fake data. "]},{"cell_type":"code","metadata":{"id":"j0_NAoyVdYTv","colab_type":"code","colab":{}},"source":["class VAE(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        \n","        self.fc1 = nn.Linear(no_features, 200)\n","        self.fc2 = nn.Linear(200, 150)\n","        \n","        #Two layers of latent variables to be used in reparametrization\n","        self.fc31 = nn.Linear(150, 100) # 100 latent variables\n","        self.fc32 = nn.Linear(150, 100) # 100 latent variables\n","        \n","        self.fc4 = nn.Linear(100, 150)\n","        \n","        self.fc5 = nn.Linear(150, 200)\n","        self.fc6 = nn.Linear(200, no_features)  \n","        \n","        self.dropout = nn.Dropout(p=0.25)\n","        \n","    # Create latent variables (return 2 for further use)\n","    def encode(self, x):\n","        h1 = self.dropout(F.relu(self.fc1(x)))\n","        h1 = self.dropout(F.relu(self.fc2(h1)))\n","        \n","        # What is returned is mu and logvar, mean and var that will be used to cacl std\n","        return self.dropout(self.fc31(h1)), self.dropout(self.fc32(h1))\n","    \n","    # Count parameters \n","    def reparameterize(self, mu, logvar):\n","        std = t.exp(0.5*logvar)\n","        eps = t.randn_like(std)\n","        \n","        return mu + eps*std\n","    \n","    # Change latent variables to output\n","    def decode(self, z):\n","        h2 = self.dropout(F.relu(self.fc4(z)))\n","        h2 = self.dropout(F.relu(self.fc5(h2)))\n","        \n","        return t.sigmoid(self.fc6(h2))\n","    \n","    #Encode, parametrize, decode and return in one method\n","    def forward(self, x):\n","        mu, logvar = self.encode(x)\n","        z = self.reparameterize(mu, logvar)\n","        \n","        return self.decode(z), mu, logvar\n","        \n","#Loss function to be used with VAE, I dont quite get it yet, but it works :v\n","def loss_function(recon_x, x, mu, logvar):\n","    #Generation loss - compare generated to obtained\n","    gen_loss_func = nn.MSELoss(reduction='sum')\n","    GL = gen_loss_func(recon_x, x)  \n","    # KL divergence that measures how closely the latent variables match a unit gaussian.\n","    #Simplified version\n","    KLD = -0.5 * t.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    return GL + KLD      \n","        \n","# Put net to model object\n","model = VAE()\n","if gpu:\n","    model.cuda()\n","# Predefined loss function\n","criterion = loss_function\n","# Optimizer - Adam\n","optimizer = optim.Adam(model.parameters(), lr=0.003)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Nr4oH5wHUfdd","colab_type":"text"},"source":["##7.Quick-Test Model Validity\n","Is it possible to pass data throught the model witout error?"]},{"cell_type":"code","metadata":{"id":"J41S-Qy5Ui3I","colab_type":"code","outputId":"2a3bc11f-481c-4c65-c379-f70594353334","executionInfo":{"status":"ok","timestamp":1559067441627,"user_tz":-120,"elapsed":8527,"user":{"displayName":"Bartłomiej Cerek","photoUrl":"https://lh6.googleusercontent.com/-63SSrSJFeXQ/AAAAAAAAAAI/AAAAAAAAF5A/_OvOPODWwwg/s64/photo.jpg","userId":"03865172822680659937"}},"colab":{"base_uri":"https://localhost:8080/","height":173}},"source":["# Load some variable for test (we dont care about labels)\n","features, labels = next(iter(test_gen))\n","if gpu:\n","    features = features.cuda()\n","\n","#Check if size is correct, should be batch_size x no_features\n","print(features.shape)\n","\n","# Show values after passing throught untrained network \n","model.forward(features)[:1]"],"execution_count":0,"outputs":[{"output_type":"stream","text":["torch.Size([10, 239])\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["(tensor([[0.4642, 0.4597, 0.5074,  ..., 0.4546, 0.4651, 0.4588],\n","         [0.4897, 0.5348, 0.4714,  ..., 0.5050, 0.4956, 0.4337],\n","         [0.4890, 0.4424, 0.4900,  ..., 0.5246, 0.5128, 0.5281],\n","         ...,\n","         [0.4616, 0.4957, 0.5184,  ..., 0.5157, 0.5163, 0.4849],\n","         [0.4722, 0.4894, 0.4727,  ..., 0.4972, 0.5092, 0.4960],\n","         [0.4973, 0.4346, 0.4796,  ..., 0.5404, 0.4643, 0.4538]],\n","        device='cuda:0', grad_fn=<SigmoidBackward>),)"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"SNlMUiVhVh2x","colab_type":"text"},"source":["## 8.Train the network"]},{"cell_type":"code","metadata":{"id":"-Fw1QlN8VjS0","colab_type":"code","outputId":"5fe91212-63c9-4ccf-97a5-72d4d2965447","executionInfo":{"status":"ok","timestamp":1559067650144,"user_tz":-120,"elapsed":217034,"user":{"displayName":"Bartłomiej Cerek","photoUrl":"https://lh6.googleusercontent.com/-63SSrSJFeXQ/AAAAAAAAAAI/AAAAAAAAF5A/_OvOPODWwwg/s64/photo.jpg","userId":"03865172822680659937"}},"colab":{"base_uri":"https://localhost:8080/","height":2011}},"source":["# number of epochs\n","n_epochs = 100\n","\n","valid_loss_min = np.Inf # to track change in validation loss\n","\n","# Iterate on epochs \n","for epoch in range(1, n_epochs+1):\n","\n","    # keep track of training and validation loss\n","    train_loss = 0.0\n","    valid_loss = 0.0\n","\n","    # Set model to train mode (to include dropout)\n","    model.train()\n","    \n","    # iterate on data batches, discard labels\n","    for features, _ in train_gen:\n","        # move tensors to GPU if CUDA is available\n","        if gpu:\n","            features = features.cuda()\n","        # clear the gradients of all optimized variables\n","        optimizer.zero_grad()\n","        # forward pass: compute predicted outputs by passing inputs throught the model\n","        output, mu, logvar = model.forward(features)\n","        # calculate the batch loss by compering to initial features\n","        loss = criterion(output, features, mu, logvar)\n","        # backward pass: compute gradient of the loss with respect to model parameters\n","        loss.backward()\n","        # perform a single optimization step (parameter update)\n","        optimizer.step()\n","        # update training loss , mulitply by batchsize, no idea why\n","        train_loss += loss.item() *features.size(0)\n","          \n","    # Validate the model \n","    \n","    # Set model to evaluation mode tu use its full power\n","    model.eval()\n","    \n","    for features, _ in valid_gen:\n","        # move tensors to GPU if CUDA is available\n","        if gpu:\n","            features = features.cuda()\n","        # forward pass: compute predicted outputs by passing inputs to the model\n","        output, mu, logvar = model.forward(features)\n","        # calculate the batch loss with predefined function\n","        loss = criterion(output, features, mu, logvar)\n","        # update average validation loss, mulitply by batchsize, no idea why\n","        valid_loss += loss.item() *features.size(0)\n","    \n","    # calculate average losses\n","    train_loss = train_loss/len(train_gen.sampler)\n","    valid_loss = valid_loss/len(valid_gen.sampler)\n","    \n","        \n","    # print training/validation statistics \n","    print('Epoch: {} \\tTraining Loss: {:.4f} \\tValidation Loss: {:.6f}'.format(epoch, train_loss, valid_loss))\n","    \n","    # save model if validation loss has decreased\n","    if valid_loss <= valid_loss_min:\n","        print('Validation loss decreased ({:.4f} --> {:.4f}).  Saving model ...'.format(valid_loss_min, valid_loss))\n","        t.save(model.state_dict(), proj_folder + 'vae_model.pt')\n","        valid_loss_min = valid_loss"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch: 1 \tTraining Loss: 96.4353 \tValidation Loss: 80.703179\n","Validation loss decreased (inf --> 80.7032).  Saving model ...\n","Epoch: 2 \tTraining Loss: 81.6170 \tValidation Loss: 80.123016\n","Validation loss decreased (80.7032 --> 80.1230).  Saving model ...\n","Epoch: 3 \tTraining Loss: 81.1212 \tValidation Loss: 80.489704\n","Epoch: 4 \tTraining Loss: 80.8285 \tValidation Loss: 80.432178\n","Epoch: 5 \tTraining Loss: 80.6371 \tValidation Loss: 81.459358\n","Epoch: 6 \tTraining Loss: 80.7031 \tValidation Loss: 79.645190\n","Validation loss decreased (80.1230 --> 79.6452).  Saving model ...\n","Epoch: 7 \tTraining Loss: 80.4213 \tValidation Loss: 79.874479\n","Epoch: 8 \tTraining Loss: 80.4374 \tValidation Loss: 79.523975\n","Validation loss decreased (79.6452 --> 79.5240).  Saving model ...\n","Epoch: 9 \tTraining Loss: 80.3282 \tValidation Loss: 79.974641\n","Epoch: 10 \tTraining Loss: 80.1900 \tValidation Loss: 79.801690\n","Epoch: 11 \tTraining Loss: 80.2786 \tValidation Loss: 79.457586\n","Validation loss decreased (79.5240 --> 79.4576).  Saving model ...\n","Epoch: 12 \tTraining Loss: 80.1528 \tValidation Loss: 79.634148\n","Epoch: 13 \tTraining Loss: 80.2311 \tValidation Loss: 79.557561\n","Epoch: 14 \tTraining Loss: 80.0617 \tValidation Loss: 79.572063\n","Epoch: 15 \tTraining Loss: 80.0000 \tValidation Loss: 79.726367\n","Epoch: 16 \tTraining Loss: 80.0629 \tValidation Loss: 79.646402\n","Epoch: 17 \tTraining Loss: 80.0712 \tValidation Loss: 79.552677\n","Epoch: 18 \tTraining Loss: 79.9348 \tValidation Loss: 79.389314\n","Validation loss decreased (79.4576 --> 79.3893).  Saving model ...\n","Epoch: 19 \tTraining Loss: 79.9671 \tValidation Loss: 79.536140\n","Epoch: 20 \tTraining Loss: 79.9258 \tValidation Loss: 79.741349\n","Epoch: 21 \tTraining Loss: 79.7973 \tValidation Loss: 79.154521\n","Validation loss decreased (79.3893 --> 79.1545).  Saving model ...\n","Epoch: 22 \tTraining Loss: 79.8265 \tValidation Loss: 79.617115\n","Epoch: 23 \tTraining Loss: 79.7696 \tValidation Loss: 79.438820\n","Epoch: 24 \tTraining Loss: 79.8321 \tValidation Loss: 79.170688\n","Epoch: 25 \tTraining Loss: 79.8609 \tValidation Loss: 79.713589\n","Epoch: 26 \tTraining Loss: 79.7601 \tValidation Loss: 79.814578\n","Epoch: 27 \tTraining Loss: 79.7624 \tValidation Loss: 79.591027\n","Epoch: 28 \tTraining Loss: 79.6655 \tValidation Loss: 79.139699\n","Validation loss decreased (79.1545 --> 79.1397).  Saving model ...\n","Epoch: 29 \tTraining Loss: 79.6543 \tValidation Loss: 79.448918\n","Epoch: 30 \tTraining Loss: 79.5663 \tValidation Loss: 79.113680\n","Validation loss decreased (79.1397 --> 79.1137).  Saving model ...\n","Epoch: 31 \tTraining Loss: 79.5816 \tValidation Loss: 79.172129\n","Epoch: 32 \tTraining Loss: 79.5497 \tValidation Loss: 79.250360\n","Epoch: 33 \tTraining Loss: 79.6172 \tValidation Loss: 79.332068\n","Epoch: 34 \tTraining Loss: 79.6862 \tValidation Loss: 79.224561\n","Epoch: 35 \tTraining Loss: 79.5785 \tValidation Loss: 79.474775\n","Epoch: 36 \tTraining Loss: 79.6439 \tValidation Loss: 79.370652\n","Epoch: 37 \tTraining Loss: 79.5646 \tValidation Loss: 79.447890\n","Epoch: 38 \tTraining Loss: 79.5870 \tValidation Loss: 79.329441\n","Epoch: 39 \tTraining Loss: 79.6132 \tValidation Loss: 79.323832\n","Epoch: 40 \tTraining Loss: 79.5905 \tValidation Loss: 79.409337\n","Epoch: 41 \tTraining Loss: 79.6563 \tValidation Loss: 79.070179\n","Validation loss decreased (79.1137 --> 79.0702).  Saving model ...\n","Epoch: 42 \tTraining Loss: 79.4921 \tValidation Loss: 79.356387\n","Epoch: 43 \tTraining Loss: 79.3840 \tValidation Loss: 79.279390\n","Epoch: 44 \tTraining Loss: 79.5829 \tValidation Loss: 79.330274\n","Epoch: 45 \tTraining Loss: 79.4577 \tValidation Loss: 79.139485\n","Epoch: 46 \tTraining Loss: 79.4851 \tValidation Loss: 79.136461\n","Epoch: 47 \tTraining Loss: 79.4540 \tValidation Loss: 79.390061\n","Epoch: 48 \tTraining Loss: 79.3752 \tValidation Loss: 79.011408\n","Validation loss decreased (79.0702 --> 79.0114).  Saving model ...\n","Epoch: 49 \tTraining Loss: 79.4924 \tValidation Loss: 79.088757\n","Epoch: 50 \tTraining Loss: 79.4695 \tValidation Loss: 79.252447\n","Epoch: 51 \tTraining Loss: 79.4657 \tValidation Loss: 79.150466\n","Epoch: 52 \tTraining Loss: 79.4632 \tValidation Loss: 79.457590\n","Epoch: 53 \tTraining Loss: 79.4318 \tValidation Loss: 78.997323\n","Validation loss decreased (79.0114 --> 78.9973).  Saving model ...\n","Epoch: 54 \tTraining Loss: 79.5024 \tValidation Loss: 78.963326\n","Validation loss decreased (78.9973 --> 78.9633).  Saving model ...\n","Epoch: 55 \tTraining Loss: 79.3885 \tValidation Loss: 79.351027\n","Epoch: 56 \tTraining Loss: 79.4490 \tValidation Loss: 79.341953\n","Epoch: 57 \tTraining Loss: 79.3726 \tValidation Loss: 79.137273\n","Epoch: 58 \tTraining Loss: 79.3367 \tValidation Loss: 79.253894\n","Epoch: 59 \tTraining Loss: 79.3670 \tValidation Loss: 79.147762\n","Epoch: 60 \tTraining Loss: 79.3766 \tValidation Loss: 79.343635\n","Epoch: 61 \tTraining Loss: 79.2732 \tValidation Loss: 79.223606\n","Epoch: 62 \tTraining Loss: 79.2684 \tValidation Loss: 78.992857\n","Epoch: 63 \tTraining Loss: 79.2558 \tValidation Loss: 79.059895\n","Epoch: 64 \tTraining Loss: 79.3117 \tValidation Loss: 79.212619\n","Epoch: 65 \tTraining Loss: 79.2377 \tValidation Loss: 79.175421\n","Epoch: 66 \tTraining Loss: 79.2616 \tValidation Loss: 79.251972\n","Epoch: 67 \tTraining Loss: 79.2864 \tValidation Loss: 79.119668\n","Epoch: 68 \tTraining Loss: 79.2589 \tValidation Loss: 79.187303\n","Epoch: 69 \tTraining Loss: 79.2485 \tValidation Loss: 79.078468\n","Epoch: 70 \tTraining Loss: 79.2250 \tValidation Loss: 79.329539\n","Epoch: 71 \tTraining Loss: 79.2436 \tValidation Loss: 79.182572\n","Epoch: 72 \tTraining Loss: 79.2216 \tValidation Loss: 79.116430\n","Epoch: 73 \tTraining Loss: 79.1835 \tValidation Loss: 79.135168\n","Epoch: 74 \tTraining Loss: 79.2314 \tValidation Loss: 79.188632\n","Epoch: 75 \tTraining Loss: 79.2075 \tValidation Loss: 79.007814\n","Epoch: 76 \tTraining Loss: 79.2053 \tValidation Loss: 79.022685\n","Epoch: 77 \tTraining Loss: 79.1979 \tValidation Loss: 79.072881\n","Epoch: 78 \tTraining Loss: 79.2113 \tValidation Loss: 79.209325\n","Epoch: 79 \tTraining Loss: 79.1962 \tValidation Loss: 79.037899\n","Epoch: 80 \tTraining Loss: 79.1715 \tValidation Loss: 78.989656\n","Epoch: 81 \tTraining Loss: 79.1508 \tValidation Loss: 79.205869\n","Epoch: 82 \tTraining Loss: 79.1652 \tValidation Loss: 79.079808\n","Epoch: 83 \tTraining Loss: 79.1216 \tValidation Loss: 79.084154\n","Epoch: 84 \tTraining Loss: 79.1127 \tValidation Loss: 79.204974\n","Epoch: 85 \tTraining Loss: 79.1197 \tValidation Loss: 79.131540\n","Epoch: 86 \tTraining Loss: 79.1118 \tValidation Loss: 79.055206\n","Epoch: 87 \tTraining Loss: 79.1263 \tValidation Loss: 78.965362\n","Epoch: 88 \tTraining Loss: 79.0971 \tValidation Loss: 79.001990\n","Epoch: 89 \tTraining Loss: 79.1052 \tValidation Loss: 78.942200\n","Validation loss decreased (78.9633 --> 78.9422).  Saving model ...\n","Epoch: 90 \tTraining Loss: 79.0847 \tValidation Loss: 79.050616\n","Epoch: 91 \tTraining Loss: 79.0902 \tValidation Loss: 78.932600\n","Validation loss decreased (78.9422 --> 78.9326).  Saving model ...\n","Epoch: 92 \tTraining Loss: 79.1044 \tValidation Loss: 79.147909\n","Epoch: 93 \tTraining Loss: 79.1393 \tValidation Loss: 79.137400\n","Epoch: 94 \tTraining Loss: 79.0834 \tValidation Loss: 79.132697\n","Epoch: 95 \tTraining Loss: 79.1227 \tValidation Loss: 78.966814\n","Epoch: 96 \tTraining Loss: 79.1086 \tValidation Loss: 79.146473\n","Epoch: 97 \tTraining Loss: 79.0943 \tValidation Loss: 79.072706\n","Epoch: 98 \tTraining Loss: 79.1058 \tValidation Loss: 79.025430\n","Epoch: 99 \tTraining Loss: 79.0728 \tValidation Loss: 79.079771\n","Epoch: 100 \tTraining Loss: 79.0849 \tValidation Loss: 79.142754\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"s8sIc-roy0ix","colab_type":"text"},"source":["## 9.Test and score counting\n","Load model which had smallest validation loss in training. \n","\n","**Count score**\n","\n","Score is single value that allows for model evaluation. In this case it is counted as follows: \n","For test batch that contains same amount of flagged and not flagged samples obtain Mean Square Error of each sample and model output after forwarding that sample. \n","\n","Then if sample was **flagged** add its MSE to sum value and if it was **not flagged** subtract it. Obtained sum divide by number of samples in test batch to discard size effect. We do it because if MSE is **big** for flagged cases its good but its bad if it is high for correct samples.\n","\n","Higher the score is, better performance of the model.\n","\n","*Silly but, hey, it kind of works :v*\n","\n","**Also collect all losses, to be used in next step**\n"]},{"cell_type":"code","metadata":{"id":"2rqpNUNwXn8j","colab_type":"code","outputId":"eb1f37d1-3038-4bcc-e671-bb0b187581d7","executionInfo":{"status":"ok","timestamp":1559067650713,"user_tz":-120,"elapsed":217594,"user":{"displayName":"Bartłomiej Cerek","photoUrl":"https://lh6.googleusercontent.com/-63SSrSJFeXQ/AAAAAAAAAAI/AAAAAAAAF5A/_OvOPODWwwg/s64/photo.jpg","userId":"03865172822680659937"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["#Load model with best parameters\n","model.load_state_dict(t.load(proj_folder + 'vae_model.pt'))\n","\n","# track test loss\n","loss_sum = 0.0\n","loss_lab_list = []\n","\n","# Mean Square Error criterion for test purposes\n","criterion = nn.MSELoss()\n","\n","# Set model to evaluation to use its full power\n","model.eval()\n","# iterate over test data\n","for features, labels in test_gen:\n","    # move tensors to GPU if CUDA is available\n","    if gpu:\n","       features, labels = features.cuda(), labels.cuda()\n","    # forward pass: compute predicted outputs by passing inputs to the model\n","    output, mu, logvar = model.forward(features)\n","\n","    # Update test loss for score purposes\n","    for samp_no, lab in enumerate(labels):\n","        loss = criterion(output[samp_no], features[samp_no])\n","        # Create list of losses and labels for next step\n","        loss_lab_list.append((loss.item(), lab.item()))\n","        if lab.item(): # true if flagged\n","            loss_sum += loss.item()\n","        else:\n","            loss_sum -= loss.item()\n","\n","# Print average MSE\n","score = loss_sum / len(test_gen.sampler)\n","print('MSE score of test: %.7f' % score )\n","print('(Multiplied by 1k) %.7f' % (score *1000) )"],"execution_count":0,"outputs":[{"output_type":"stream","text":["MSE score of test: 0.0218929\n","(Multiplied by 1k) 21.8928791\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9oo6l2eBTOtS","colab_type":"text"},"source":["##10. Check distributions\n","\n","Analyze the MSE distributions."]},{"cell_type":"code","metadata":{"id":"a1RkqkN7VCCk","colab_type":"code","outputId":"1a19c8b4-6cbf-4ce4-f9ca-b96fce191eae","executionInfo":{"status":"ok","timestamp":1559067650970,"user_tz":-120,"elapsed":217845,"user":{"displayName":"Bartłomiej Cerek","photoUrl":"https://lh6.googleusercontent.com/-63SSrSJFeXQ/AAAAAAAAAAI/AAAAAAAAF5A/_OvOPODWwwg/s64/photo.jpg","userId":"03865172822680659937"}},"colab":{"base_uri":"https://localhost:8080/","height":563}},"source":["#Load model with best parameters\n","model.load_state_dict(t.load(proj_folder + 'vae_model.pt'))\n","\n","#Create Data frame for box chart \n","loss_lab = list(zip(*loss_lab_list))\n","dist_loss = np.array(list(loss_lab[0]))\n","dist_lab = np.array(list(loss_lab[1]))\n","\n","distr = pd.DataFrame(columns = ['Flagged','Not Flagged'])\n","distr['Flagged'] = dist_loss[dist_lab == 1]\n","distr['Not Flagged'] = dist_loss[dist_lab != 1]\n","\n","#Visualisation\n","print('MSE distribution in test samples:')\n","distr.boxplot()\n","distr.describe()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["MSE distribution in test samples:\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Flagged</th>\n","      <th>Not Flagged</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>70.000000</td>\n","      <td>70.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>0.077171</td>\n","      <td>0.033385</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>0.059560</td>\n","      <td>0.006948</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>0.025047</td>\n","      <td>0.022053</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>0.034061</td>\n","      <td>0.029929</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>0.061093</td>\n","      <td>0.032729</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>0.089144</td>\n","      <td>0.035092</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>0.304763</td>\n","      <td>0.062013</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         Flagged  Not Flagged\n","count  70.000000    70.000000\n","mean    0.077171     0.033385\n","std     0.059560     0.006948\n","min     0.025047     0.022053\n","25%     0.034061     0.029929\n","50%     0.061093     0.032729\n","75%     0.089144     0.035092\n","max     0.304763     0.062013"]},"metadata":{"tags":[]},"execution_count":20},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nfyYMiAVBA15ZAAAAAElFTkSuQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"q4dGmlRTdY5D","colab_type":"text"},"source":["## 11.Decide boundary \n","To implement classificator, boundary value must be selected. Now it's done manually then some rule can be established.\n","\n","**Falsly flagged** are preferd over **falsly not flagged**.  \n","\n","Boundry can be selected either to maximize correct classifications either to safely classifie more samples as flagged."]},{"cell_type":"code","metadata":{"id":"9nEj44cadYGn","colab_type":"code","outputId":"b302e466-f8ce-4e12-f724-408cd87c200c","executionInfo":{"status":"ok","timestamp":1559067650971,"user_tz":-120,"elapsed":217840,"user":{"displayName":"Bartłomiej Cerek","photoUrl":"https://lh6.googleusercontent.com/-63SSrSJFeXQ/AAAAAAAAAAI/AAAAAAAAF5A/_OvOPODWwwg/s64/photo.jpg","userId":"03865172822680659937"}},"colab":{"base_uri":"https://localhost:8080/","height":87}},"source":["# Select bondary value as quantile of Flagged Values\n","q = .3 # Magic numeber out of hat <- to be replaced by rule \n","class_boundary = distr['Flagged'].quantile(q)\n","print('Boundry selected as quantile %.2f of Flagged.' % q)\n","\n","#Check how many flagged classified correctly \n","c1 = distr['Flagged'][distr['Flagged'] >= class_boundary]\n","print('Flagged correctly cassified: %d/%d ' % (len(c1), len(distr['Flagged'])) )\n","\n","#Check how many not flagged classified correctly \n","c2 = distr['Not Flagged'][distr['Not Flagged'] < class_boundary]\n","print('Not Flagged correctly cassified: %d/%d ' % (len(c2), len(distr['Not Flagged'])) )\n","\n","#Total classification results\n","print('Total correctly cassified: %d/%d ' % (len(c2)+len(c1), 2*len(distr['Not Flagged'])) )"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Boundry selected as quantile 0.30 of Flagged.\n","Flagged correctly cassified: 49/70 \n","Not Flagged correctly cassified: 55/70 \n","Total correctly cassified: 104/140 \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Gt_2UcW2m57_","colab_type":"text"},"source":["## 12.Example of explain function\n","Example of explain function that could classify samples using model and explain which features influenced the choice. "]},{"cell_type":"code","metadata":{"id":"_ZEH27NbnaXL","colab_type":"code","outputId":"b0909345-44cc-4313-e0b6-1c42238a2aa2","executionInfo":{"status":"ok","timestamp":1559067650972,"user_tz":-120,"elapsed":217835,"user":{"displayName":"Bartłomiej Cerek","photoUrl":"https://lh6.googleusercontent.com/-63SSrSJFeXQ/AAAAAAAAAAI/AAAAAAAAF5A/_OvOPODWwwg/s64/photo.jpg","userId":"03865172822680659937"}},"colab":{"base_uri":"https://localhost:8080/","height":254}},"source":["#Get random flagged sample  \n","r = np.random.choice(f_indices)\n","#X is from preprocessing step of creating dataset. It's tensor of normalized data.\n","exp_sample = X[r]\n","if gpu:\n","    exp_sample = exp_sample.cuda()\n","    \n","model.eval()\n","output, mu, logvar = model.forward(exp_sample)\n","loss = criterion(output, exp_sample)\n","\n","print('MSE value for sample: %.7f with boundary %.7f' % (loss, class_boundary))\n","\n","if loss >= class_boundary: \n","    print('This sample is classified as FLAGGED.')\n","else:\n","    print('This sample is classified as NOT FLAGGED.')\n","\n","#Get features that differ the most\n","feat_order = pd.DataFrame(columns=['Col','Feat_diff'])\n","feat_order['Col'] = filtered.columns\n","feat_order['Feat_diff'] = abs(exp_sample.cpu().data.numpy() - output.cpu().data.numpy())\n","feat_order = feat_order.sort_values('Feat_diff', ascending = False)\n","\n","print('Features that differ the most from expected values are:')\n","feat_order.head()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["MSE value for sample: 0.2113732 with boundary 0.0355261\n","This sample is classified as FLAGGED.\n","Features that differ the most from expected values are:\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Col</th>\n","      <th>Feat_diff</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>131</th>\n","      <td>deltaPt</td>\n","      <td>1.198630</td>\n","    </tr>\n","    <tr>\n","      <th>133</th>\n","      <td>deltaPtA</td>\n","      <td>1.007080</td>\n","    </tr>\n","    <tr>\n","      <th>128</th>\n","      <td>qOverPt</td>\n","      <td>1.001850</td>\n","    </tr>\n","    <tr>\n","      <th>217</th>\n","      <td>zPull</td>\n","      <td>1.000035</td>\n","    </tr>\n","    <tr>\n","      <th>152</th>\n","      <td>dcaz_posA_1_Err</td>\n","      <td>0.976191</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                 Col  Feat_diff\n","131          deltaPt   1.198630\n","133         deltaPtA   1.007080\n","128          qOverPt   1.001850\n","217            zPull   1.000035\n","152  dcaz_posA_1_Err   0.976191"]},"metadata":{"tags":[]},"execution_count":22}]}]}