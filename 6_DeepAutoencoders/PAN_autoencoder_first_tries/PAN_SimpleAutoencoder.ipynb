{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PAN_SimpleAutoencoder.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"LCQBk-cXMR1R","colab_type":"text"},"source":["# Simple autoencoder\n","\n","## 1.Import modules and set paths\n","Modify here to load files from different places.\n","\n","All used libraries are here."]},{"cell_type":"code","metadata":{"id":"c4hXmiQRMEBt","colab_type":"code","outputId":"473c844e-db80-4470-c906-a623bc1b3741","executionInfo":{"status":"ok","timestamp":1567224945657,"user_tz":-540,"elapsed":908,"user":{"displayName":"Bartłomiej Cerek","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBCye5yJ9tW1vLbOr7P95dVuwBX69OragQhxO_BMQ=s64","userId":"03865172822680659937"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Mount gdrive\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","# Data managment \n","import pandas as pd \n","import numpy as np \n","from sklearn.preprocessing import MaxAbsScaler, MinMaxScaler\n","\n","import torch as t\n","from torch import nn, optim\n","from torch.utils import data as data_lib\n","import torch.nn.functional as F\n","\n","# Utilities\n","import os\n","from sklearn import preprocessing\n","\n","proj_folder = '/content/gdrive/My Drive/ColabNotebooks/PAN_autoencoder_first_tries/'\n","data_folder = proj_folder + 'data/'\n","solutions_folder = proj_folder + 'data/solutions/'"],"execution_count":55,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"P3IDH-swNjml","colab_type":"text"},"source":["## 2.Load and select data\n","In this model **alias_global_Outlier** will be used as label."]},{"cell_type":"code","metadata":{"id":"PKubEOzzOYzr","colab_type":"code","colab":{}},"source":["%%capture\n","#Get RAW Data\n","if not os.path.isfile('trending_merged_LHC18f_withGraphs.csv'):\n","    if not os.path.isfile('CERNdata.zip'):\n","        !wget 'https://cernbox.cern.ch/index.php/s/JnxbQZ0nzITGC4i/download' -O CERNdata.zip\n","    !7z x CERNdata.zip"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8OoSTrXbxEZp","colab_type":"code","colab":{}},"source":["#Proton-Proton\n","raw_f = pd.read_csv('trending_merged_LHC18f_withGraphs.csv')\n","raw_f['period'] = 'LHC18f'\n","raw_o = pd.read_csv('trending_merged_LHC18o_withGraphs.csv')\n","raw_o['period'] = 'LHC18o'\n","raw_p = pd.read_csv('trending_merged_LHC18p_withGraphs.csv')\n","raw_p['period'] = 'LHC18p'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oUmPZlx6xLLw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":270},"outputId":"352b7ae8-6ef3-48ab-f600-d58fbf2c4031","executionInfo":{"status":"ok","timestamp":1567224954754,"user_tz":-540,"elapsed":9887,"user":{"displayName":"Bartłomiej Cerek","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBCye5yJ9tW1vLbOr7P95dVuwBX69OragQhxO_BMQ=s64","userId":"03865172822680659937"}}},"source":["raw = pd.concat([raw_f, raw_o, raw_p])\n","raw.head()"],"execution_count":58,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>run</th>\n","      <th>chunkID</th>\n","      <th>time</th>\n","      <th>year</th>\n","      <th>period.fString</th>\n","      <th>pass.fString</th>\n","      <th>dataType.fString</th>\n","      <th>startTimeGRP</th>\n","      <th>stopTimeGRP</th>\n","      <th>duration</th>\n","      <th>bz</th>\n","      <th>runType.fString</th>\n","      <th>meanTPCnclF</th>\n","      <th>rmsTPCnclF</th>\n","      <th>meanTPCChi2</th>\n","      <th>rmsTPCChi2</th>\n","      <th>slopeATPCnclF</th>\n","      <th>slopeCTPCnclF</th>\n","      <th>slopeATPCnclFErr</th>\n","      <th>slopeCTPCnclFErr</th>\n","      <th>meanTPCncl</th>\n","      <th>rmsTPCncl</th>\n","      <th>slopeATPCncl</th>\n","      <th>slopeCTPCncl</th>\n","      <th>slopeATPCnclErr</th>\n","      <th>slopeCTPCnclErr</th>\n","      <th>hasRawQA</th>\n","      <th>rawClusterCounter</th>\n","      <th>rawSignalCounter</th>\n","      <th>offsetdZA</th>\n","      <th>slopedZA</th>\n","      <th>offsetdZC</th>\n","      <th>slopedZC</th>\n","      <th>offsetdZAErr</th>\n","      <th>slopedZAErr</th>\n","      <th>offsetdZCErr</th>\n","      <th>slopedZCErr</th>\n","      <th>offsetdZAchi2</th>\n","      <th>slopedZAchi2</th>\n","      <th>...</th>\n","      <th>grdcaz_neg_CSidePhi_Y_106</th>\n","      <th>grdcaz_neg_CSidePhi_Y_107</th>\n","      <th>grdcaz_neg_CSidePhi_Y_108</th>\n","      <th>grdcaz_neg_CSidePhi_Y_109</th>\n","      <th>grdcaz_neg_CSidePhi_Y_110</th>\n","      <th>grdcaz_neg_CSidePhi_Y_111</th>\n","      <th>grdcaz_neg_CSidePhi_Y_112</th>\n","      <th>grdcaz_neg_CSidePhi_Y_113</th>\n","      <th>grdcaz_neg_CSidePhi_Y_114</th>\n","      <th>grdcaz_neg_CSidePhi_Y_115</th>\n","      <th>grdcaz_neg_CSidePhi_Y_116</th>\n","      <th>grdcaz_neg_CSidePhi_Y_117</th>\n","      <th>grdcaz_neg_CSidePhi_Y_118</th>\n","      <th>grdcaz_neg_CSidePhi_Y_119</th>\n","      <th>grdcaz_neg_CSidePhi_Y_120</th>\n","      <th>grdcaz_neg_CSidePhi_Y_121</th>\n","      <th>grdcaz_neg_CSidePhi_Y_122</th>\n","      <th>grdcaz_neg_CSidePhi_Y_123</th>\n","      <th>grdcaz_neg_CSidePhi_Y_124</th>\n","      <th>grdcaz_neg_CSidePhi_Y_125</th>\n","      <th>grdcaz_neg_CSidePhi_Y_126</th>\n","      <th>grdcaz_neg_CSidePhi_Y_127</th>\n","      <th>grdcaz_neg_CSidePhi_Y_128</th>\n","      <th>grdcaz_neg_CSidePhi_Y_129</th>\n","      <th>grdcaz_neg_CSidePhi_Y_130</th>\n","      <th>grdcaz_neg_CSidePhi_Y_131</th>\n","      <th>grdcaz_neg_CSidePhi_Y_132</th>\n","      <th>grdcaz_neg_CSidePhi_Y_133</th>\n","      <th>grdcaz_neg_CSidePhi_Y_134</th>\n","      <th>grdcaz_neg_CSidePhi_Y_135</th>\n","      <th>grdcaz_neg_CSidePhi_Y_136</th>\n","      <th>grdcaz_neg_CSidePhi_Y_137</th>\n","      <th>grdcaz_neg_CSidePhi_Y_138</th>\n","      <th>grdcaz_neg_CSidePhi_Y_139</th>\n","      <th>grdcaz_neg_CSidePhi_Y_140</th>\n","      <th>grdcaz_neg_CSidePhi_Y_141</th>\n","      <th>grdcaz_neg_CSidePhi_Y_142</th>\n","      <th>grdcaz_neg_CSidePhi_Y_143</th>\n","      <th>interactionRate</th>\n","      <th>period</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>287000</td>\n","      <td>1</td>\n","      <td>1527204124</td>\n","      <td>2018</td>\n","      <td>LHC18f</td>\n","      <td>pass1</td>\n","      <td>NaN</td>\n","      <td>1527204124</td>\n","      <td>1527231797</td>\n","      <td>27673</td>\n","      <td>-0.499998</td>\n","      <td>PHYSICS</td>\n","      <td>0.913711</td>\n","      <td>0.091254</td>\n","      <td>1.767776</td>\n","      <td>0.446999</td>\n","      <td>-0.003785</td>\n","      <td>0.000947</td>\n","      <td>0.000674</td>\n","      <td>0.000947</td>\n","      <td>133.596942</td>\n","      <td>22.675932</td>\n","      <td>-0.061015</td>\n","      <td>-0.029429</td>\n","      <td>0.172744</td>\n","      <td>-0.029429</td>\n","      <td>1</td>\n","      <td>367302864</td>\n","      <td>547136439</td>\n","      <td>0.008433</td>\n","      <td>0.061533</td>\n","      <td>-0.095885</td>\n","      <td>0.060379</td>\n","      <td>0.002402</td>\n","      <td>0.005589</td>\n","      <td>0.002441</td>\n","      <td>0.005575</td>\n","      <td>54.538913</td>\n","      <td>54.538913</td>\n","      <td>...</td>\n","      <td>-0.052315</td>\n","      <td>-0.058496</td>\n","      <td>-0.057705</td>\n","      <td>-0.007200</td>\n","      <td>-0.066442</td>\n","      <td>-0.079369</td>\n","      <td>-0.117361</td>\n","      <td>-0.061361</td>\n","      <td>-0.099326</td>\n","      <td>-0.078831</td>\n","      <td>-0.097516</td>\n","      <td>-0.107208</td>\n","      <td>-0.109287</td>\n","      <td>-0.102642</td>\n","      <td>-0.066415</td>\n","      <td>-0.069376</td>\n","      <td>-0.061681</td>\n","      <td>-0.096441</td>\n","      <td>-0.055511</td>\n","      <td>-0.059102</td>\n","      <td>-0.068315</td>\n","      <td>-0.102027</td>\n","      <td>-0.086352</td>\n","      <td>-0.070171</td>\n","      <td>-0.066603</td>\n","      <td>-0.059391</td>\n","      <td>-0.111429</td>\n","      <td>-0.073658</td>\n","      <td>-0.090766</td>\n","      <td>-0.029640</td>\n","      <td>-0.084562</td>\n","      <td>-0.079724</td>\n","      <td>-0.069284</td>\n","      <td>-0.045167</td>\n","      <td>-0.065301</td>\n","      <td>-0.050480</td>\n","      <td>-0.050551</td>\n","      <td>-0.069094</td>\n","      <td>195584.670984</td>\n","      <td>LHC18f</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>287000</td>\n","      <td>2</td>\n","      <td>1527204124</td>\n","      <td>2018</td>\n","      <td>LHC18f</td>\n","      <td>pass1</td>\n","      <td>NaN</td>\n","      <td>1527204124</td>\n","      <td>1527231797</td>\n","      <td>27673</td>\n","      <td>-0.499998</td>\n","      <td>PHYSICS</td>\n","      <td>0.913795</td>\n","      <td>0.091471</td>\n","      <td>1.765181</td>\n","      <td>0.446966</td>\n","      <td>-0.004339</td>\n","      <td>0.000274</td>\n","      <td>0.000660</td>\n","      <td>0.000274</td>\n","      <td>133.604316</td>\n","      <td>22.668970</td>\n","      <td>-0.227194</td>\n","      <td>-0.046269</td>\n","      <td>0.170329</td>\n","      <td>-0.046269</td>\n","      <td>1</td>\n","      <td>367302864</td>\n","      <td>547136439</td>\n","      <td>0.007302</td>\n","      <td>0.050437</td>\n","      <td>0.041850</td>\n","      <td>0.239221</td>\n","      <td>0.002347</td>\n","      <td>0.005482</td>\n","      <td>0.002514</td>\n","      <td>0.005686</td>\n","      <td>80.242746</td>\n","      <td>80.242746</td>\n","      <td>...</td>\n","      <td>-0.013811</td>\n","      <td>-0.043341</td>\n","      <td>-0.036767</td>\n","      <td>-0.008760</td>\n","      <td>-0.059732</td>\n","      <td>-0.010744</td>\n","      <td>-0.024306</td>\n","      <td>-0.056385</td>\n","      <td>-0.063628</td>\n","      <td>-0.059174</td>\n","      <td>-0.067915</td>\n","      <td>-0.013919</td>\n","      <td>-0.018712</td>\n","      <td>-0.042576</td>\n","      <td>-0.029183</td>\n","      <td>-0.065051</td>\n","      <td>-0.074004</td>\n","      <td>-0.062315</td>\n","      <td>-0.098015</td>\n","      <td>-0.032065</td>\n","      <td>-0.039085</td>\n","      <td>-0.037202</td>\n","      <td>-0.020365</td>\n","      <td>-0.052420</td>\n","      <td>-0.092947</td>\n","      <td>-0.017293</td>\n","      <td>-0.044535</td>\n","      <td>-0.009809</td>\n","      <td>-0.026729</td>\n","      <td>-0.048561</td>\n","      <td>-0.006055</td>\n","      <td>-0.034508</td>\n","      <td>-0.051278</td>\n","      <td>-0.038319</td>\n","      <td>0.017724</td>\n","      <td>0.012698</td>\n","      <td>-0.032471</td>\n","      <td>-0.020261</td>\n","      <td>195584.670984</td>\n","      <td>LHC18f</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>287000</td>\n","      <td>3</td>\n","      <td>1527204124</td>\n","      <td>2018</td>\n","      <td>LHC18f</td>\n","      <td>pass1</td>\n","      <td>NaN</td>\n","      <td>1527204124</td>\n","      <td>1527231797</td>\n","      <td>27673</td>\n","      <td>-0.499998</td>\n","      <td>PHYSICS</td>\n","      <td>0.913980</td>\n","      <td>0.091291</td>\n","      <td>1.766238</td>\n","      <td>0.447530</td>\n","      <td>-0.003400</td>\n","      <td>0.000098</td>\n","      <td>0.000655</td>\n","      <td>0.000098</td>\n","      <td>133.619454</td>\n","      <td>22.658997</td>\n","      <td>-0.226418</td>\n","      <td>-0.220931</td>\n","      <td>0.169301</td>\n","      <td>-0.220931</td>\n","      <td>1</td>\n","      <td>367302864</td>\n","      <td>547136439</td>\n","      <td>0.018170</td>\n","      <td>0.035006</td>\n","      <td>0.122846</td>\n","      <td>0.356713</td>\n","      <td>0.002312</td>\n","      <td>0.005374</td>\n","      <td>0.002513</td>\n","      <td>0.005630</td>\n","      <td>84.160363</td>\n","      <td>84.160363</td>\n","      <td>...</td>\n","      <td>-0.005258</td>\n","      <td>0.001520</td>\n","      <td>-0.004679</td>\n","      <td>0.067023</td>\n","      <td>0.020974</td>\n","      <td>-0.022892</td>\n","      <td>-0.021963</td>\n","      <td>-0.054784</td>\n","      <td>-0.065607</td>\n","      <td>-0.046039</td>\n","      <td>-0.033981</td>\n","      <td>-0.057274</td>\n","      <td>-0.023231</td>\n","      <td>-0.018649</td>\n","      <td>0.002054</td>\n","      <td>-0.031758</td>\n","      <td>-0.085111</td>\n","      <td>-0.042000</td>\n","      <td>0.024556</td>\n","      <td>-0.053818</td>\n","      <td>-0.004125</td>\n","      <td>-0.005745</td>\n","      <td>0.006034</td>\n","      <td>-0.042845</td>\n","      <td>-0.028715</td>\n","      <td>0.006710</td>\n","      <td>-0.015597</td>\n","      <td>-0.024007</td>\n","      <td>-0.001764</td>\n","      <td>-0.029742</td>\n","      <td>-0.010322</td>\n","      <td>-0.021278</td>\n","      <td>0.031273</td>\n","      <td>0.034793</td>\n","      <td>0.030032</td>\n","      <td>0.017758</td>\n","      <td>-0.000799</td>\n","      <td>0.018394</td>\n","      <td>195584.670984</td>\n","      <td>LHC18f</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>287000</td>\n","      <td>4</td>\n","      <td>1527204124</td>\n","      <td>2018</td>\n","      <td>LHC18f</td>\n","      <td>pass1</td>\n","      <td>NaN</td>\n","      <td>1527204124</td>\n","      <td>1527231797</td>\n","      <td>27673</td>\n","      <td>-0.499998</td>\n","      <td>PHYSICS</td>\n","      <td>0.913710</td>\n","      <td>0.091409</td>\n","      <td>1.769212</td>\n","      <td>0.448276</td>\n","      <td>-0.003525</td>\n","      <td>-0.000843</td>\n","      <td>0.000665</td>\n","      <td>-0.000843</td>\n","      <td>133.561797</td>\n","      <td>22.654738</td>\n","      <td>-0.160837</td>\n","      <td>-0.230723</td>\n","      <td>0.169628</td>\n","      <td>-0.230723</td>\n","      <td>1</td>\n","      <td>367302864</td>\n","      <td>547136439</td>\n","      <td>0.010970</td>\n","      <td>0.044343</td>\n","      <td>0.186074</td>\n","      <td>0.444474</td>\n","      <td>0.002419</td>\n","      <td>0.005634</td>\n","      <td>0.002601</td>\n","      <td>0.005870</td>\n","      <td>60.973159</td>\n","      <td>60.973159</td>\n","      <td>...</td>\n","      <td>0.014411</td>\n","      <td>0.009355</td>\n","      <td>0.031501</td>\n","      <td>0.053673</td>\n","      <td>0.053667</td>\n","      <td>-0.022732</td>\n","      <td>-0.014178</td>\n","      <td>0.003725</td>\n","      <td>-0.034899</td>\n","      <td>-0.010383</td>\n","      <td>-0.003872</td>\n","      <td>-0.062970</td>\n","      <td>-0.029144</td>\n","      <td>-0.003223</td>\n","      <td>-0.035575</td>\n","      <td>-0.022201</td>\n","      <td>0.005223</td>\n","      <td>-0.007628</td>\n","      <td>-0.009116</td>\n","      <td>-0.012818</td>\n","      <td>0.022805</td>\n","      <td>-0.030858</td>\n","      <td>0.037694</td>\n","      <td>-0.013457</td>\n","      <td>-0.050117</td>\n","      <td>0.018854</td>\n","      <td>-0.016786</td>\n","      <td>0.051484</td>\n","      <td>0.024478</td>\n","      <td>0.014763</td>\n","      <td>0.042977</td>\n","      <td>-0.013600</td>\n","      <td>-0.012279</td>\n","      <td>0.023137</td>\n","      <td>-0.002161</td>\n","      <td>0.004983</td>\n","      <td>-0.024167</td>\n","      <td>0.032266</td>\n","      <td>195584.670984</td>\n","      <td>LHC18f</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>287000</td>\n","      <td>5</td>\n","      <td>1527204124</td>\n","      <td>2018</td>\n","      <td>LHC18f</td>\n","      <td>pass1</td>\n","      <td>NaN</td>\n","      <td>1527204124</td>\n","      <td>1527231797</td>\n","      <td>27673</td>\n","      <td>-0.499998</td>\n","      <td>PHYSICS</td>\n","      <td>0.913875</td>\n","      <td>0.091523</td>\n","      <td>1.769435</td>\n","      <td>0.450377</td>\n","      <td>-0.003099</td>\n","      <td>0.000421</td>\n","      <td>0.000660</td>\n","      <td>0.000421</td>\n","      <td>133.612447</td>\n","      <td>22.691172</td>\n","      <td>0.166529</td>\n","      <td>-0.192079</td>\n","      <td>0.169401</td>\n","      <td>-0.192079</td>\n","      <td>1</td>\n","      <td>367302864</td>\n","      <td>547136439</td>\n","      <td>0.005303</td>\n","      <td>0.055199</td>\n","      <td>0.078894</td>\n","      <td>0.316983</td>\n","      <td>0.002298</td>\n","      <td>0.005316</td>\n","      <td>0.002517</td>\n","      <td>0.005623</td>\n","      <td>60.250242</td>\n","      <td>60.250242</td>\n","      <td>...</td>\n","      <td>-0.012090</td>\n","      <td>-0.002313</td>\n","      <td>0.034599</td>\n","      <td>-0.001945</td>\n","      <td>-0.029631</td>\n","      <td>-0.035595</td>\n","      <td>-0.092407</td>\n","      <td>-0.070246</td>\n","      <td>-0.036920</td>\n","      <td>-0.056874</td>\n","      <td>-0.055545</td>\n","      <td>-0.042682</td>\n","      <td>-0.085999</td>\n","      <td>-0.029165</td>\n","      <td>-0.040633</td>\n","      <td>-0.055388</td>\n","      <td>-0.069295</td>\n","      <td>-0.023070</td>\n","      <td>-0.030833</td>\n","      <td>-0.030712</td>\n","      <td>-0.045799</td>\n","      <td>-0.028000</td>\n","      <td>-0.040346</td>\n","      <td>-0.032017</td>\n","      <td>-0.029775</td>\n","      <td>-0.015439</td>\n","      <td>-0.002545</td>\n","      <td>-0.038118</td>\n","      <td>-0.034014</td>\n","      <td>-0.017843</td>\n","      <td>-0.022103</td>\n","      <td>-0.058039</td>\n","      <td>-0.033304</td>\n","      <td>0.007296</td>\n","      <td>-0.003022</td>\n","      <td>-0.033521</td>\n","      <td>0.010312</td>\n","      <td>-0.017453</td>\n","      <td>195584.670984</td>\n","      <td>LHC18f</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 7227 columns</p>\n","</div>"],"text/plain":["   Unnamed: 0     run  ...  interactionRate  period\n","0           0  287000  ...    195584.670984  LHC18f\n","1           1  287000  ...    195584.670984  LHC18f\n","2           2  287000  ...    195584.670984  LHC18f\n","3           3  287000  ...    195584.670984  LHC18f\n","4           4  287000  ...    195584.670984  LHC18f\n","\n","[5 rows x 7227 columns]"]},"metadata":{"tags":[]},"execution_count":58}]},{"cell_type":"code","metadata":{"id":"j-GsEzNdNmjy","colab_type":"code","outputId":"4f1fb245-b86c-4d8b-e702-aabc7c328fa5","executionInfo":{"status":"ok","timestamp":1567224954756,"user_tz":-540,"elapsed":9877,"user":{"displayName":"Bartłomiej Cerek","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBCye5yJ9tW1vLbOr7P95dVuwBX69OragQhxO_BMQ=s64","userId":"03865172822680659937"}},"colab":{"base_uri":"https://localhost:8080/","height":270}},"source":["#Define meta data, graphs, aliases\n","meta_cols = ['period', 'run', 'chunkID', 'time', 'year', 'period.fString', 'pass.fString', \n","'runType.fString', 'startTimeGRP', 'stopTimeGRP', 'duration', 'chunkStart', \n","'chunkStop', 'chunkMean', 'chunkMedian', 'chunkRMS', \n","'chunkDuration'] + ['Unnamed: 0', 'dataType.fString' ] # After + my opinion\n","\n","graph_cols = [col for col in raw if col.startswith('gr')] # +6k\n","\n","alias_cols = [col for col in raw if col.startswith('alias_')] #265\n","\n","#Point to label columns\n","war_col = 'alias_global_Warning'\n","out_col = 'alias_global_Outlier'\n","\n","period = raw['period']\n","run = raw['run']\n","chunkID = raw['chunkID']\n","\n","# Filter out data not used for classification.\n","filtered = raw.drop(meta_cols + graph_cols + alias_cols, axis=1)\n","no_samples, no_features = filtered.shape\n","\n","#Check shape\n","print(no_samples, no_features)\n","filtered.head()"],"execution_count":59,"outputs":[{"output_type":"stream","text":["3429 239\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>bz</th>\n","      <th>meanTPCnclF</th>\n","      <th>rmsTPCnclF</th>\n","      <th>meanTPCChi2</th>\n","      <th>rmsTPCChi2</th>\n","      <th>slopeATPCnclF</th>\n","      <th>slopeCTPCnclF</th>\n","      <th>slopeATPCnclFErr</th>\n","      <th>slopeCTPCnclFErr</th>\n","      <th>meanTPCncl</th>\n","      <th>rmsTPCncl</th>\n","      <th>slopeATPCncl</th>\n","      <th>slopeCTPCncl</th>\n","      <th>slopeATPCnclErr</th>\n","      <th>slopeCTPCnclErr</th>\n","      <th>hasRawQA</th>\n","      <th>rawClusterCounter</th>\n","      <th>rawSignalCounter</th>\n","      <th>offsetdZA</th>\n","      <th>slopedZA</th>\n","      <th>offsetdZC</th>\n","      <th>slopedZC</th>\n","      <th>offsetdZAErr</th>\n","      <th>slopedZAErr</th>\n","      <th>offsetdZCErr</th>\n","      <th>slopedZCErr</th>\n","      <th>offsetdZAchi2</th>\n","      <th>slopedZAchi2</th>\n","      <th>offsetdZCchi2</th>\n","      <th>slopedZCchi2</th>\n","      <th>offsetdZAPos</th>\n","      <th>slopedZAPos</th>\n","      <th>offsetdZCPos</th>\n","      <th>slopedZCPos</th>\n","      <th>offsetdZAErrPos</th>\n","      <th>slopedZAErrPos</th>\n","      <th>offsetdZCErrPos</th>\n","      <th>slopedZCErrPos</th>\n","      <th>offsetdZAchi2Pos</th>\n","      <th>slopedZAchi2Pos</th>\n","      <th>...</th>\n","      <th>oroc_C_side</th>\n","      <th>MIPattachSlopeC</th>\n","      <th>MIPattachSlopeA</th>\n","      <th>resolutionMIP</th>\n","      <th>meanMIP</th>\n","      <th>meanMIPele</th>\n","      <th>resolutionMIPele</th>\n","      <th>electroMIPSeparation</th>\n","      <th>tpcItsMatchA</th>\n","      <th>tpcItsMatchHighPtA</th>\n","      <th>tpcItsMatchC</th>\n","      <th>tpcItsMatchHighPtC</th>\n","      <th>phiPull</th>\n","      <th>phiPullHighPt</th>\n","      <th>ptPull</th>\n","      <th>ptPullHighPt</th>\n","      <th>yPull</th>\n","      <th>yPullHighPt</th>\n","      <th>zPull</th>\n","      <th>zPullHighPt</th>\n","      <th>lambdaPull</th>\n","      <th>lambdaPullHighPt</th>\n","      <th>tpcConstrainPhiA</th>\n","      <th>tpcConstrainPhiC</th>\n","      <th>meanPTRelativeA</th>\n","      <th>medianPTRelativeA</th>\n","      <th>rmsPTRelativeA</th>\n","      <th>meanPTRelativeC</th>\n","      <th>medianPTRelativeC</th>\n","      <th>rmsPTRelativeC</th>\n","      <th>meanHVandPTGainCorrIROC</th>\n","      <th>medianHVandPTGainCorrIROC</th>\n","      <th>rmsHVandPTGainCorrIROC</th>\n","      <th>meanHVandPTGainCorrOROC</th>\n","      <th>medianHVandPTGainCorrOROC</th>\n","      <th>rmsHVandPTGainCorrOROC</th>\n","      <th>meanVDriftCorr</th>\n","      <th>medianVDriftCorr</th>\n","      <th>rmsVDriftCorr</th>\n","      <th>interactionRate</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>-0.499998</td>\n","      <td>0.913711</td>\n","      <td>0.091254</td>\n","      <td>1.767776</td>\n","      <td>0.446999</td>\n","      <td>-0.003785</td>\n","      <td>0.000947</td>\n","      <td>0.000674</td>\n","      <td>0.000947</td>\n","      <td>133.596942</td>\n","      <td>22.675932</td>\n","      <td>-0.061015</td>\n","      <td>-0.029429</td>\n","      <td>0.172744</td>\n","      <td>-0.029429</td>\n","      <td>1</td>\n","      <td>367302864</td>\n","      <td>547136439</td>\n","      <td>0.008433</td>\n","      <td>0.061533</td>\n","      <td>-0.095885</td>\n","      <td>0.060379</td>\n","      <td>0.002402</td>\n","      <td>0.005589</td>\n","      <td>0.002441</td>\n","      <td>0.005575</td>\n","      <td>54.538913</td>\n","      <td>54.538913</td>\n","      <td>1447.258711</td>\n","      <td>1447.258711</td>\n","      <td>0.011558</td>\n","      <td>0.084150</td>\n","      <td>-0.093813</td>\n","      <td>0.087700</td>\n","      <td>0.003431</td>\n","      <td>0.008177</td>\n","      <td>0.003531</td>\n","      <td>0.008089</td>\n","      <td>9.050547</td>\n","      <td>9.050547</td>\n","      <td>...</td>\n","      <td>18</td>\n","      <td>0.030036</td>\n","      <td>-0.173266</td>\n","      <td>0.064220</td>\n","      <td>49.792557</td>\n","      <td>80.735855</td>\n","      <td>0.055183</td>\n","      <td>30.943298</td>\n","      <td>0.699384</td>\n","      <td>0.710692</td>\n","      <td>0.694623</td>\n","      <td>0.703207</td>\n","      <td>-0.000119</td>\n","      <td>-0.000389</td>\n","      <td>0.527906</td>\n","      <td>0.738438</td>\n","      <td>-0.092014</td>\n","      <td>-0.210392</td>\n","      <td>-0.088328</td>\n","      <td>-0.156770</td>\n","      <td>0.119385</td>\n","      <td>0.068962</td>\n","      <td>0.017818</td>\n","      <td>-0.049315</td>\n","      <td>0.003437</td>\n","      <td>0.003433</td>\n","      <td>0.000011</td>\n","      <td>0.003560</td>\n","      <td>0.003556</td>\n","      <td>0.000011</td>\n","      <td>1.128413</td>\n","      <td>1.128399</td>\n","      <td>0.000043</td>\n","      <td>1.128413</td>\n","      <td>1.128399</td>\n","      <td>0.000043</td>\n","      <td>-0.027221</td>\n","      <td>-0.027222</td>\n","      <td>3.005739e-06</td>\n","      <td>195584.670984</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>-0.499998</td>\n","      <td>0.913795</td>\n","      <td>0.091471</td>\n","      <td>1.765181</td>\n","      <td>0.446966</td>\n","      <td>-0.004339</td>\n","      <td>0.000274</td>\n","      <td>0.000660</td>\n","      <td>0.000274</td>\n","      <td>133.604316</td>\n","      <td>22.668970</td>\n","      <td>-0.227194</td>\n","      <td>-0.046269</td>\n","      <td>0.170329</td>\n","      <td>-0.046269</td>\n","      <td>1</td>\n","      <td>367302864</td>\n","      <td>547136439</td>\n","      <td>0.007302</td>\n","      <td>0.050437</td>\n","      <td>0.041850</td>\n","      <td>0.239221</td>\n","      <td>0.002347</td>\n","      <td>0.005482</td>\n","      <td>0.002514</td>\n","      <td>0.005686</td>\n","      <td>80.242746</td>\n","      <td>80.242746</td>\n","      <td>2298.826852</td>\n","      <td>2298.826852</td>\n","      <td>0.006364</td>\n","      <td>0.079816</td>\n","      <td>0.040957</td>\n","      <td>0.262827</td>\n","      <td>0.003490</td>\n","      <td>0.008128</td>\n","      <td>0.003665</td>\n","      <td>0.008344</td>\n","      <td>35.407343</td>\n","      <td>35.407343</td>\n","      <td>...</td>\n","      <td>18</td>\n","      <td>-0.046077</td>\n","      <td>-0.174695</td>\n","      <td>0.064034</td>\n","      <td>49.808353</td>\n","      <td>80.820270</td>\n","      <td>0.055974</td>\n","      <td>31.011913</td>\n","      <td>0.698263</td>\n","      <td>0.696498</td>\n","      <td>0.692948</td>\n","      <td>0.703005</td>\n","      <td>-0.000016</td>\n","      <td>0.001378</td>\n","      <td>0.533350</td>\n","      <td>0.784235</td>\n","      <td>-0.053545</td>\n","      <td>-0.088791</td>\n","      <td>-0.077231</td>\n","      <td>-0.174282</td>\n","      <td>0.118407</td>\n","      <td>0.125998</td>\n","      <td>0.008589</td>\n","      <td>-0.031984</td>\n","      <td>0.003476</td>\n","      <td>0.003474</td>\n","      <td>0.000017</td>\n","      <td>0.003597</td>\n","      <td>0.003596</td>\n","      <td>0.000017</td>\n","      <td>1.128563</td>\n","      <td>1.128559</td>\n","      <td>0.000066</td>\n","      <td>1.128563</td>\n","      <td>1.128559</td>\n","      <td>0.000066</td>\n","      <td>-0.027226</td>\n","      <td>-0.027226</td>\n","      <td>8.138511e-07</td>\n","      <td>195584.670984</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-0.499998</td>\n","      <td>0.913980</td>\n","      <td>0.091291</td>\n","      <td>1.766238</td>\n","      <td>0.447530</td>\n","      <td>-0.003400</td>\n","      <td>0.000098</td>\n","      <td>0.000655</td>\n","      <td>0.000098</td>\n","      <td>133.619454</td>\n","      <td>22.658997</td>\n","      <td>-0.226418</td>\n","      <td>-0.220931</td>\n","      <td>0.169301</td>\n","      <td>-0.220931</td>\n","      <td>1</td>\n","      <td>367302864</td>\n","      <td>547136439</td>\n","      <td>0.018170</td>\n","      <td>0.035006</td>\n","      <td>0.122846</td>\n","      <td>0.356713</td>\n","      <td>0.002312</td>\n","      <td>0.005374</td>\n","      <td>0.002513</td>\n","      <td>0.005630</td>\n","      <td>84.160363</td>\n","      <td>84.160363</td>\n","      <td>2689.649719</td>\n","      <td>2689.649719</td>\n","      <td>0.018739</td>\n","      <td>0.060694</td>\n","      <td>0.122070</td>\n","      <td>0.383957</td>\n","      <td>0.003383</td>\n","      <td>0.007934</td>\n","      <td>0.003700</td>\n","      <td>0.008220</td>\n","      <td>64.929950</td>\n","      <td>64.929950</td>\n","      <td>...</td>\n","      <td>18</td>\n","      <td>-0.040411</td>\n","      <td>-0.132918</td>\n","      <td>0.063939</td>\n","      <td>49.698433</td>\n","      <td>80.666520</td>\n","      <td>0.055939</td>\n","      <td>30.968086</td>\n","      <td>0.700643</td>\n","      <td>0.707197</td>\n","      <td>0.693301</td>\n","      <td>0.706140</td>\n","      <td>-0.000104</td>\n","      <td>0.000041</td>\n","      <td>0.534116</td>\n","      <td>0.760895</td>\n","      <td>-0.058528</td>\n","      <td>-0.119211</td>\n","      <td>-0.078093</td>\n","      <td>-0.172614</td>\n","      <td>0.118760</td>\n","      <td>0.088854</td>\n","      <td>0.012813</td>\n","      <td>-0.016630</td>\n","      <td>0.003457</td>\n","      <td>0.003462</td>\n","      <td>0.000018</td>\n","      <td>0.003578</td>\n","      <td>0.003583</td>\n","      <td>0.000018</td>\n","      <td>1.128491</td>\n","      <td>1.128508</td>\n","      <td>0.000068</td>\n","      <td>1.128491</td>\n","      <td>1.128508</td>\n","      <td>0.000068</td>\n","      <td>-0.027230</td>\n","      <td>-0.027230</td>\n","      <td>2.510162e-06</td>\n","      <td>195584.670984</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>-0.499998</td>\n","      <td>0.913710</td>\n","      <td>0.091409</td>\n","      <td>1.769212</td>\n","      <td>0.448276</td>\n","      <td>-0.003525</td>\n","      <td>-0.000843</td>\n","      <td>0.000665</td>\n","      <td>-0.000843</td>\n","      <td>133.561797</td>\n","      <td>22.654738</td>\n","      <td>-0.160837</td>\n","      <td>-0.230723</td>\n","      <td>0.169628</td>\n","      <td>-0.230723</td>\n","      <td>1</td>\n","      <td>367302864</td>\n","      <td>547136439</td>\n","      <td>0.010970</td>\n","      <td>0.044343</td>\n","      <td>0.186074</td>\n","      <td>0.444474</td>\n","      <td>0.002419</td>\n","      <td>0.005634</td>\n","      <td>0.002601</td>\n","      <td>0.005870</td>\n","      <td>60.973159</td>\n","      <td>60.973159</td>\n","      <td>2930.115067</td>\n","      <td>2930.115067</td>\n","      <td>0.011269</td>\n","      <td>0.076198</td>\n","      <td>0.189182</td>\n","      <td>0.477827</td>\n","      <td>0.003562</td>\n","      <td>0.008437</td>\n","      <td>0.003797</td>\n","      <td>0.008594</td>\n","      <td>38.295893</td>\n","      <td>38.295893</td>\n","      <td>...</td>\n","      <td>18</td>\n","      <td>-0.011914</td>\n","      <td>-0.215666</td>\n","      <td>0.064052</td>\n","      <td>49.721306</td>\n","      <td>80.747475</td>\n","      <td>0.055767</td>\n","      <td>31.026169</td>\n","      <td>0.693546</td>\n","      <td>0.691877</td>\n","      <td>0.686828</td>\n","      <td>0.708658</td>\n","      <td>-0.000176</td>\n","      <td>0.000900</td>\n","      <td>0.535176</td>\n","      <td>0.826308</td>\n","      <td>-0.044240</td>\n","      <td>-0.047569</td>\n","      <td>-0.071471</td>\n","      <td>-0.156174</td>\n","      <td>0.111323</td>\n","      <td>0.044026</td>\n","      <td>0.005626</td>\n","      <td>-0.008393</td>\n","      <td>0.003319</td>\n","      <td>0.003297</td>\n","      <td>0.000066</td>\n","      <td>0.003439</td>\n","      <td>0.003418</td>\n","      <td>0.000066</td>\n","      <td>1.127953</td>\n","      <td>1.127870</td>\n","      <td>0.000259</td>\n","      <td>1.127953</td>\n","      <td>1.127870</td>\n","      <td>0.000259</td>\n","      <td>-0.027235</td>\n","      <td>-0.027235</td>\n","      <td>6.439609e-07</td>\n","      <td>195584.670984</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>-0.499998</td>\n","      <td>0.913875</td>\n","      <td>0.091523</td>\n","      <td>1.769435</td>\n","      <td>0.450377</td>\n","      <td>-0.003099</td>\n","      <td>0.000421</td>\n","      <td>0.000660</td>\n","      <td>0.000421</td>\n","      <td>133.612447</td>\n","      <td>22.691172</td>\n","      <td>0.166529</td>\n","      <td>-0.192079</td>\n","      <td>0.169401</td>\n","      <td>-0.192079</td>\n","      <td>1</td>\n","      <td>367302864</td>\n","      <td>547136439</td>\n","      <td>0.005303</td>\n","      <td>0.055199</td>\n","      <td>0.078894</td>\n","      <td>0.316983</td>\n","      <td>0.002298</td>\n","      <td>0.005316</td>\n","      <td>0.002517</td>\n","      <td>0.005623</td>\n","      <td>60.250242</td>\n","      <td>60.250242</td>\n","      <td>3570.688486</td>\n","      <td>3570.688486</td>\n","      <td>0.004176</td>\n","      <td>0.086193</td>\n","      <td>0.068185</td>\n","      <td>0.317270</td>\n","      <td>0.003333</td>\n","      <td>0.007839</td>\n","      <td>0.003675</td>\n","      <td>0.008233</td>\n","      <td>32.077150</td>\n","      <td>32.077150</td>\n","      <td>...</td>\n","      <td>18</td>\n","      <td>-0.020808</td>\n","      <td>-0.182842</td>\n","      <td>0.064080</td>\n","      <td>49.760740</td>\n","      <td>80.831540</td>\n","      <td>0.055385</td>\n","      <td>31.070805</td>\n","      <td>0.699800</td>\n","      <td>0.693445</td>\n","      <td>0.694479</td>\n","      <td>0.688442</td>\n","      <td>-0.000039</td>\n","      <td>0.000240</td>\n","      <td>0.533761</td>\n","      <td>0.793600</td>\n","      <td>-0.057473</td>\n","      <td>-0.166837</td>\n","      <td>-0.081095</td>\n","      <td>-0.152884</td>\n","      <td>0.112461</td>\n","      <td>0.054255</td>\n","      <td>0.009307</td>\n","      <td>-0.040449</td>\n","      <td>0.003297</td>\n","      <td>0.003287</td>\n","      <td>0.000037</td>\n","      <td>0.003417</td>\n","      <td>0.003407</td>\n","      <td>0.000037</td>\n","      <td>1.127865</td>\n","      <td>1.127825</td>\n","      <td>0.000142</td>\n","      <td>1.127865</td>\n","      <td>1.127825</td>\n","      <td>0.000142</td>\n","      <td>-0.027237</td>\n","      <td>-0.027236</td>\n","      <td>1.405422e-06</td>\n","      <td>195584.670984</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 239 columns</p>\n","</div>"],"text/plain":["         bz  meanTPCnclF  ...  rmsVDriftCorr  interactionRate\n","0 -0.499998     0.913711  ...   3.005739e-06    195584.670984\n","1 -0.499998     0.913795  ...   8.138511e-07    195584.670984\n","2 -0.499998     0.913980  ...   2.510162e-06    195584.670984\n","3 -0.499998     0.913710  ...   6.439609e-07    195584.670984\n","4 -0.499998     0.913875  ...   1.405422e-06    195584.670984\n","\n","[5 rows x 239 columns]"]},"metadata":{"tags":[]},"execution_count":59}]},{"cell_type":"markdown","metadata":{"id":"I_QycZXOi0tS","colab_type":"text"},"source":["## 3.Create and shuffle indices\n","Also check amount of flagged and not flagged"]},{"cell_type":"code","metadata":{"id":"YsVUyHx0Y-V4","colab_type":"code","outputId":"ed28dcfc-6831-4529-89fb-f7518448fe41","executionInfo":{"status":"ok","timestamp":1567224954757,"user_tz":-540,"elapsed":9864,"user":{"displayName":"Bartłomiej Cerek","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBCye5yJ9tW1vLbOr7P95dVuwBX69OragQhxO_BMQ=s64","userId":"03865172822680659937"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["# Always there is warning when there is error!\n","\n","# [0] becasue silly 'where' returns touple\n","crit = raw[war_col].astype(bool)\n","f_indices = np.where(crit)[0] # Flagged indices\n","no_f_indices = np.where(~crit)[0] # Not flagged indices\n","\n","# Number of flagged elements\n","print(len(f_indices))\n","# Number of not flagged elements\n","print(len(no_f_indices))\n","\n","#Shuffle intances to ensure that with each run different samples are drawn\n","random_seed= 42 # Controlled randomnes\n","np.random.seed(random_seed)\n","np.random.shuffle(f_indices)\n","np.random.shuffle(no_f_indices)"],"execution_count":60,"outputs":[{"output_type":"stream","text":["167\n","3262\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-01U0zg-aIJ0","colab_type":"text"},"source":["## 4.Preprocess and create generators.\n","Generators will be used for training, validation and test.\n","\n","Some specific features of PyTorch are used."]},{"cell_type":"code","metadata":{"id":"EvBT55eubIDk","colab_type":"code","outputId":"21a43ea7-1e48-4e81-fcea-bece10f5e16d","executionInfo":{"status":"ok","timestamp":1567224955060,"user_tz":-540,"elapsed":10158,"user":{"displayName":"Bartłomiej Cerek","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBCye5yJ9tW1vLbOr7P95dVuwBX69OragQhxO_BMQ=s64","userId":"03865172822680659937"}},"colab":{"base_uri":"https://localhost:8080/","height":153}},"source":["#Parameters\n","params = {'batch_size': 16,\n","          'num_workers': 8}\n","scaler = MaxAbsScaler()\n","# scaler = MinMaxScaler()\n","\n","# 10% of all correct instances will be used as validation\n","valid_split = int(np.floor(.1 * len(no_f_indices))) \n","# 70 correct and 70 flagged samples will be used as test\n","test_split = 70\n","\n","#Create dataset\n","X = t.FloatTensor(scaler.fit_transform(filtered)) #features\n","Y = t.IntTensor(raw[war_col].to_numpy()) #labels\n","dataset = data_lib.TensorDataset(X,Y)\n","\n","#Set training indices of correct cases\n","test_indices_c, valid_indices, train_indices = np.split(no_f_indices, [test_split, test_split+valid_split])\n","# Add flagged indices to test indices \n","test_indices = np.concatenate((test_indices_c, f_indices[:test_split]))\n","\n","# Create samplers\n","train_sampler = data_lib.SubsetRandomSampler(train_indices)\n","valid_sampler = data_lib.SubsetRandomSampler(valid_indices)\n","test_sampler = data_lib.SubsetRandomSampler(test_indices)\n","\n","#Create generators\n","train_gen = data_lib.DataLoader(dataset, **params,sampler=train_sampler)\n","valid_gen = data_lib.DataLoader(dataset, **params, sampler=valid_sampler)\n","test_gen = data_lib.DataLoader(dataset, **params, sampler=test_sampler)\n","\n","#Check if all fine\n","next(iter(test_gen))"],"execution_count":61,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[tensor([[ 1.0000,  0.9844,  0.8326,  ..., -0.2704,  0.2757,  0.3150],\n","         [ 1.0000,  0.9920,  0.8611,  ..., -0.5093,  0.3584,  0.0765],\n","         [-1.0000,  0.9856,  0.8122,  ..., -0.7395,  0.0793,  0.2339],\n","         ...,\n","         [ 1.0000,  0.9836,  0.8232,  ..., -0.4032,  0.0142,  0.3110],\n","         [-1.0000,  0.9561,  0.8409,  ..., -0.7714,  0.0084,  1.0000],\n","         [-1.0000,  0.9859,  0.8085,  ..., -0.7648,  0.1088,  0.2358]]),\n"," tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0], dtype=torch.int32)]"]},"metadata":{"tags":[]},"execution_count":61}]},{"cell_type":"markdown","metadata":{"id":"uB0TxZ2kueYX","colab_type":"text"},"source":["## 5.Defining architecture of Neural Network\n","Simple autoencoder with linear fully conected layers, dropout, relu act. i final tanh."]},{"cell_type":"code","metadata":{"id":"j0_NAoyVdYTv","colab_type":"code","colab":{}},"source":["class Autoencoder(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        \n","        self.fc1 = nn.Linear(no_features, 150)\n","        self.fc2 = nn.Linear(150, 50)\n","        \n","        self.fc3 = nn.Linear(50, 20) # 100 latent variables\n","        self.fc4 = nn.Linear(20, 50)\n","        \n","        self.fc5 = nn.Linear(50, 150)\n","        self.fc6 = nn.Linear(150, no_features)  \n","        \n","        self.dropout = nn.Dropout(p=0.25)\n","        \n","    def forward(self, x):\n","        \n","        x = self.dropout(F.relu(self.fc1(x)))\n","        x = self.dropout(F.relu(self.fc2(x)))\n","        x = self.dropout(F.relu(self.fc3(x)))\n","        x = self.dropout(F.relu(self.fc4(x)))\n","        x = self.dropout(F.relu(self.fc5(x)))\n","                               \n","        x = t.tanh(self.fc6(x)) #last act funcion tanh\n","        return x\n","\n","# Put net to model object\n","model = Autoencoder()\n","# Mean Square Error criterion\n","criterion = nn.MSELoss()\n","# Optimizer - Adam\n","optimizer = optim.Adam(model.parameters(), lr=0.003)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Nr4oH5wHUfdd","colab_type":"text"},"source":["##6.Quick-Test Model Validity\n","Is it possible to pass data throught the model witout error?"]},{"cell_type":"code","metadata":{"id":"J41S-Qy5Ui3I","colab_type":"code","outputId":"ed5a5b17-d994-4548-b0f6-69296891d91f","executionInfo":{"status":"ok","timestamp":1567224956055,"user_tz":-540,"elapsed":11127,"user":{"displayName":"Bartłomiej Cerek","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBCye5yJ9tW1vLbOr7P95dVuwBX69OragQhxO_BMQ=s64","userId":"03865172822680659937"}},"colab":{"base_uri":"https://localhost:8080/","height":136}},"source":["# Load some variable for test (we dont care about labels)\n","features, labels = next(iter(test_gen))\n","\n","#Check if size is correct, should be batch_size x no_features\n","print(features.shape)\n","\n","# Show values after passing throught untrained network \n","model.forward(features)[:5]"],"execution_count":63,"outputs":[{"output_type":"stream","text":["torch.Size([16, 239])\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0.0043,  0.0360, -0.0078,  ..., -0.0111, -0.1239,  0.0946],\n","        [ 0.0134,  0.0983,  0.0257,  ..., -0.0280, -0.0687,  0.0772],\n","        [ 0.0381,  0.0300,  0.0278,  ..., -0.0355, -0.0599,  0.0625],\n","        [ 0.0632,  0.0192,  0.0121,  ..., -0.0242, -0.0864,  0.0600],\n","        [ 0.0196,  0.0507,  0.0085,  ..., -0.0167, -0.0852,  0.1113]],\n","       grad_fn=<SliceBackward>)"]},"metadata":{"tags":[]},"execution_count":63}]},{"cell_type":"markdown","metadata":{"id":"1YstusgBr8Ss","colab_type":"text"},"source":["## 7.Prepare for training\n","\n","Check if training on GPU is possible and if yes move model."]},{"cell_type":"code","metadata":{"id":"q13i7kCqVZ_g","colab_type":"code","outputId":"5af90c4c-b076-4568-ddd4-8e635cba9452","executionInfo":{"status":"ok","timestamp":1567224956056,"user_tz":-540,"elapsed":11119,"user":{"displayName":"Bartłomiej Cerek","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBCye5yJ9tW1vLbOr7P95dVuwBX69OragQhxO_BMQ=s64","userId":"03865172822680659937"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# check if CUDA is available\n","gpu = t.cuda.is_available()\n","\n","if not gpu:\n","    print('CUDA is not available.  Training on CPU ...')\n","    model.cpu()\n","else:\n","    print('CUDA is available!  Training on GPU ...')\n","    model.cuda()"],"execution_count":64,"outputs":[{"output_type":"stream","text":["CUDA is available!  Training on GPU ...\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"SNlMUiVhVh2x","colab_type":"text"},"source":["## 8.Train the network\n","Network is being trained only on **not flagged** data!"]},{"cell_type":"code","metadata":{"id":"-Fw1QlN8VjS0","colab_type":"code","outputId":"eb9a8b10-70dd-436a-a052-73765ab6f4c1","executionInfo":{"status":"ok","timestamp":1567225062176,"user_tz":-540,"elapsed":117233,"user":{"displayName":"Bartłomiej Cerek","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBCye5yJ9tW1vLbOr7P95dVuwBX69OragQhxO_BMQ=s64","userId":"03865172822680659937"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# number of epochs\n","n_epochs = 50\n","\n","valid_loss_min = np.Inf # to track change in validation loss\n","\n","# Iterate on epochs \n","for epoch in range(1, n_epochs+1):\n","\n","    # keep track of training and validation loss\n","    train_loss = 0.0\n","    valid_loss = 0.0\n","\n","    # Set model to train mode (to include dropout)\n","    model.train()\n","    \n","    # iterate on data batches, discard labels\n","    for features, _ in train_gen:\n","        # move tensors to GPU if CUDA is available\n","        if gpu:\n","            features = features.cuda()\n","        # clear the gradients of all optimized variables\n","        optimizer.zero_grad()\n","        # forward pass: compute predicted outputs by passing inputs throught the model\n","        output = model.forward(features)\n","        # calculate the batch loss by compering to initial features\n","        loss = criterion(output, features)\n","        # backward pass: compute gradient of the loss with respect to model parameters\n","        loss.backward()\n","        # perform a single optimization step (parameter update)\n","        optimizer.step()\n","        # update average validation loss, mulitply by batchsize for bigger nums\n","        train_loss += loss.item() *features.size(0)\n","          \n","    # Validate the model \n","    \n","    # Set model to evaluation mode tu use its full power\n","    model.eval()\n","    \n","    for features, _ in valid_gen:\n","        # move tensors to GPU if CUDA is available\n","        if gpu:\n","            features = features.cuda()\n","        # forward pass: compute predicted outputs by passing inputs to the model\n","        output = model.forward(features)\n","        # calculate the batch loss by compering to initial features\n","        loss = criterion(output, features)\n","        # update average validation loss, mulitply by batchsize for bigger nums\n","        valid_loss += loss.item() *features.size(0)\n","    \n","    # calculate average losses\n","    train_loss = train_loss/len(train_gen.sampler)\n","    valid_loss = valid_loss/len(valid_gen.sampler)\n","    \n","        \n","    # print training/validation statistics \n","    print('Epoch: {} \\tTraining Loss: {:.4f} \\tValidation Loss: {:.6f}'.format(epoch, train_loss, valid_loss))\n","    \n","    # save model if validation loss has decreased\n","    if valid_loss <= valid_loss_min:\n","        print('Validation loss decreased ({:.4f} --> {:.4f}).  Saving model ...'.format(valid_loss_min, valid_loss))\n","        t.save(model.state_dict(), proj_folder + 'simple_model.pt')\n","        valid_loss_min = valid_loss"],"execution_count":65,"outputs":[{"output_type":"stream","text":["Epoch: 1 \tTraining Loss: 0.0192 \tValidation Loss: 0.005989\n","Validation loss decreased (inf --> 0.0060).  Saving model ...\n","Epoch: 2 \tTraining Loss: 0.0063 \tValidation Loss: 0.004889\n","Validation loss decreased (0.0060 --> 0.0049).  Saving model ...\n","Epoch: 3 \tTraining Loss: 0.0056 \tValidation Loss: 0.004873\n","Validation loss decreased (0.0049 --> 0.0049).  Saving model ...\n","Epoch: 4 \tTraining Loss: 0.0053 \tValidation Loss: 0.004622\n","Validation loss decreased (0.0049 --> 0.0046).  Saving model ...\n","Epoch: 5 \tTraining Loss: 0.0052 \tValidation Loss: 0.004668\n","Epoch: 6 \tTraining Loss: 0.0051 \tValidation Loss: 0.004574\n","Validation loss decreased (0.0046 --> 0.0046).  Saving model ...\n","Epoch: 7 \tTraining Loss: 0.0052 \tValidation Loss: 0.004749\n","Epoch: 8 \tTraining Loss: 0.0052 \tValidation Loss: 0.004992\n","Epoch: 9 \tTraining Loss: 0.0050 \tValidation Loss: 0.004594\n","Epoch: 10 \tTraining Loss: 0.0049 \tValidation Loss: 0.004611\n","Epoch: 11 \tTraining Loss: 0.0051 \tValidation Loss: 0.005027\n","Epoch: 12 \tTraining Loss: 0.0050 \tValidation Loss: 0.004701\n","Epoch: 13 \tTraining Loss: 0.0049 \tValidation Loss: 0.004580\n","Epoch: 14 \tTraining Loss: 0.0048 \tValidation Loss: 0.004511\n","Validation loss decreased (0.0046 --> 0.0045).  Saving model ...\n","Epoch: 15 \tTraining Loss: 0.0051 \tValidation Loss: 0.004419\n","Validation loss decreased (0.0045 --> 0.0044).  Saving model ...\n","Epoch: 16 \tTraining Loss: 0.0048 \tValidation Loss: 0.004109\n","Validation loss decreased (0.0044 --> 0.0041).  Saving model ...\n","Epoch: 17 \tTraining Loss: 0.0047 \tValidation Loss: 0.003883\n","Validation loss decreased (0.0041 --> 0.0039).  Saving model ...\n","Epoch: 18 \tTraining Loss: 0.0046 \tValidation Loss: 0.003862\n","Validation loss decreased (0.0039 --> 0.0039).  Saving model ...\n","Epoch: 19 \tTraining Loss: 0.0045 \tValidation Loss: 0.003692\n","Validation loss decreased (0.0039 --> 0.0037).  Saving model ...\n","Epoch: 20 \tTraining Loss: 0.0044 \tValidation Loss: 0.003586\n","Validation loss decreased (0.0037 --> 0.0036).  Saving model ...\n","Epoch: 21 \tTraining Loss: 0.0041 \tValidation Loss: 0.003552\n","Validation loss decreased (0.0036 --> 0.0036).  Saving model ...\n","Epoch: 22 \tTraining Loss: 0.0044 \tValidation Loss: 0.003657\n","Epoch: 23 \tTraining Loss: 0.0042 \tValidation Loss: 0.003977\n","Epoch: 24 \tTraining Loss: 0.0042 \tValidation Loss: 0.003707\n","Epoch: 25 \tTraining Loss: 0.0041 \tValidation Loss: 0.003558\n","Epoch: 26 \tTraining Loss: 0.0040 \tValidation Loss: 0.003605\n","Epoch: 27 \tTraining Loss: 0.0041 \tValidation Loss: 0.003504\n","Validation loss decreased (0.0036 --> 0.0035).  Saving model ...\n","Epoch: 28 \tTraining Loss: 0.0041 \tValidation Loss: 0.003457\n","Validation loss decreased (0.0035 --> 0.0035).  Saving model ...\n","Epoch: 29 \tTraining Loss: 0.0040 \tValidation Loss: 0.003496\n","Epoch: 30 \tTraining Loss: 0.0041 \tValidation Loss: 0.003447\n","Validation loss decreased (0.0035 --> 0.0034).  Saving model ...\n","Epoch: 31 \tTraining Loss: 0.0039 \tValidation Loss: 0.003450\n","Epoch: 32 \tTraining Loss: 0.0040 \tValidation Loss: 0.003596\n","Epoch: 33 \tTraining Loss: 0.0043 \tValidation Loss: 0.003623\n","Epoch: 34 \tTraining Loss: 0.0042 \tValidation Loss: 0.003642\n","Epoch: 35 \tTraining Loss: 0.0040 \tValidation Loss: 0.003627\n","Epoch: 36 \tTraining Loss: 0.0039 \tValidation Loss: 0.003665\n","Epoch: 37 \tTraining Loss: 0.0039 \tValidation Loss: 0.003498\n","Epoch: 38 \tTraining Loss: 0.0038 \tValidation Loss: 0.003394\n","Validation loss decreased (0.0034 --> 0.0034).  Saving model ...\n","Epoch: 39 \tTraining Loss: 0.0040 \tValidation Loss: 0.003419\n","Epoch: 40 \tTraining Loss: 0.0038 \tValidation Loss: 0.003416\n","Epoch: 41 \tTraining Loss: 0.0039 \tValidation Loss: 0.003403\n","Epoch: 42 \tTraining Loss: 0.0040 \tValidation Loss: 0.003444\n","Epoch: 43 \tTraining Loss: 0.0041 \tValidation Loss: 0.003604\n","Epoch: 44 \tTraining Loss: 0.0041 \tValidation Loss: 0.003744\n","Epoch: 45 \tTraining Loss: 0.0041 \tValidation Loss: 0.003519\n","Epoch: 46 \tTraining Loss: 0.0042 \tValidation Loss: 0.003459\n","Epoch: 47 \tTraining Loss: 0.0040 \tValidation Loss: 0.003455\n","Epoch: 48 \tTraining Loss: 0.0040 \tValidation Loss: 0.003418\n","Epoch: 49 \tTraining Loss: 0.0039 \tValidation Loss: 0.003531\n","Epoch: 50 \tTraining Loss: 0.0039 \tValidation Loss: 0.003476\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"s8sIc-roy0ix","colab_type":"text"},"source":["## 9.Test and score counting\n","Load model which had smallest validation loss in training. \n","\n","**Count score**\n","\n","Score is single value that allows for model evaluation. In this case it is counted as follows: \n","For test batch that contains same amount of flagged and not flagged samples obtain Mean Square Error of each sample and model output after forwarding that sample. \n","\n","Then if sample was **flagged** add its MSE to sum value and if it was **not flagged** subtract it. Obtained sum divide by number of samples in test batch to discard size effect. We do it because if MSE is **big** for flagged cases its good but its bad if it is high for correct samples.\n","\n","Higher the score is, better performance of the model.\n","\n","*Silly but, hey, it kind of works :v*\n","\n","**Also collect all losses, to be used in next step**\n"]},{"cell_type":"code","metadata":{"id":"2rqpNUNwXn8j","colab_type":"code","outputId":"4a3505db-670d-4063-b9a7-066fb07f7347","executionInfo":{"status":"ok","timestamp":1567225062940,"user_tz":-540,"elapsed":117991,"user":{"displayName":"Bartłomiej Cerek","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBCye5yJ9tW1vLbOr7P95dVuwBX69OragQhxO_BMQ=s64","userId":"03865172822680659937"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["#Load model with best parameters\n","model.load_state_dict(t.load(proj_folder + 'simple_model.pt'))\n","\n","# track test loss\n","loss_sum = 0.0\n","loss_lab_list = []\n","\n","# Set model to evaluation to use its full power\n","model.eval()\n","# iterate over test data\n","for features, labels in test_gen:\n","    # move tensors to GPU if CUDA is available\n","    if gpu:\n","       features, labels = features.cuda(), labels.cuda()\n","    # forward pass: compute predicted outputs by passing inputs to the model\n","    output = model.forward(features)\n","\n","    # Update test loss for score purposes\n","    for samp_no, lab in enumerate(labels):\n","        loss = criterion(output[samp_no], features[samp_no])\n","        # Create list of losses and labels for next step\n","        loss_lab_list.append((loss.item(), lab.item()))\n","        if lab.item(): # true if flagged\n","            loss_sum += loss.item()\n","        else:\n","            loss_sum -= loss.item()\n","\n","# Print average MSE\n","score = loss_sum / len(test_gen.sampler)\n","print('MSE score of test: %.7f' % score )\n","print('(Multiplied by 1k) %.7f' % (score *1000) )"],"execution_count":66,"outputs":[{"output_type":"stream","text":["MSE score of test: 0.0149130\n","(Multiplied by 1k) 14.9130073\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9oo6l2eBTOtS","colab_type":"text"},"source":["##10. Check distributions\n","\n","Analyze the MSE distributions."]},{"cell_type":"code","metadata":{"id":"a1RkqkN7VCCk","colab_type":"code","outputId":"11783c31-f33a-498d-da76-22f5967e5bdb","executionInfo":{"status":"ok","timestamp":1567225062941,"user_tz":-540,"elapsed":117985,"user":{"displayName":"Bartłomiej Cerek","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBCye5yJ9tW1vLbOr7P95dVuwBX69OragQhxO_BMQ=s64","userId":"03865172822680659937"}},"colab":{"base_uri":"https://localhost:8080/","height":566}},"source":["#Load model with best parameters\n","model.load_state_dict(t.load(proj_folder + 'simple_model.pt'))\n","\n","#Create Data frame for box chart \n","loss_lab = list(zip(*loss_lab_list))\n","dist_loss = np.array(list(loss_lab[0]))\n","dist_lab = np.array(list(loss_lab[1]))\n","\n","distr = pd.DataFrame(columns = ['Flagged','Not Flagged'])\n","distr['Flagged'] = dist_loss[dist_lab == 1]\n","distr['Not Flagged'] = dist_loss[dist_lab != 1]\n","\n","#Visualisation\n","print('MSE distribution in test samples:')\n","distr.boxplot()\n","distr.describe()"],"execution_count":67,"outputs":[{"output_type":"stream","text":["MSE distribution in test samples:\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Flagged</th>\n","      <th>Not Flagged</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>70.000000</td>\n","      <td>70.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>0.033422</td>\n","      <td>0.003596</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>0.029124</td>\n","      <td>0.002249</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>0.002119</td>\n","      <td>0.000716</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>0.013625</td>\n","      <td>0.001906</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>0.030068</td>\n","      <td>0.003229</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>0.047669</td>\n","      <td>0.004582</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>0.178734</td>\n","      <td>0.011771</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         Flagged  Not Flagged\n","count  70.000000    70.000000\n","mean    0.033422     0.003596\n","std     0.029124     0.002249\n","min     0.002119     0.000716\n","25%     0.013625     0.001906\n","50%     0.030068     0.003229\n","75%     0.047669     0.004582\n","max     0.178734     0.011771"]},"metadata":{"tags":[]},"execution_count":67},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwczMEgeCmZkB8P8BHmM/pq1RraoAAAAASUVORK5CYII=\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"q4dGmlRTdY5D","colab_type":"text"},"source":["## 11.Decide boundary \n","To implement classificator, boundary value must be selected. Now it's done manually then some rule can be established.\n","\n","**Falsly flagged** are preferd over **falsly not flagged**.  \n","\n","Boundry can be selected either to maximize correct classifications either to safely classifie more samples as flagged."]},{"cell_type":"code","metadata":{"id":"9nEj44cadYGn","colab_type":"code","outputId":"575140a5-2847-4ab8-f754-7afc724d4e8b","executionInfo":{"status":"ok","timestamp":1567225062942,"user_tz":-540,"elapsed":117979,"user":{"displayName":"Bartłomiej Cerek","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBCye5yJ9tW1vLbOr7P95dVuwBX69OragQhxO_BMQ=s64","userId":"03865172822680659937"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["# Select bondary value as quantile of Flagged Values\n","q = .3  # Magic numeber out of hat <- to be replaced by rule\n","class_boundary = distr['Flagged'].quantile(q)\n","print('Boundry selected as quantile %.2f of Flagged MSE.' % q)\n","\n","#Check how many flagged classified correctly \n","c1 = distr['Flagged'][distr['Flagged'] >= class_boundary]\n","print('Flagged correctly cassified: %d/%d ' % (len(c1), len(distr['Flagged'])) )\n","\n","#Check how many not flagged classified correctly \n","c2 = distr['Not Flagged'][distr['Not Flagged'] < class_boundary]\n","print('Not Flagged correctly cassified: %d/%d ' % (len(c2), len(distr['Not Flagged'])) )\n","\n","#Total classification results\n","print('Total correctly cassified: %d/%d ' % (len(c2)+len(c1), 2*len(distr['Not Flagged'])) )"],"execution_count":68,"outputs":[{"output_type":"stream","text":["Boundry selected as quantile 0.30 of Flagged MSE.\n","Flagged correctly cassified: 49/70 \n","Not Flagged correctly cassified: 70/70 \n","Total correctly cassified: 119/140 \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Gt_2UcW2m57_","colab_type":"text"},"source":["## 12.Example of explain function\n","Example of explain function that could classify samples using model and explain which features influenced the choice. "]},{"cell_type":"code","metadata":{"id":"_ZEH27NbnaXL","colab_type":"code","outputId":"e0acbd19-3e41-4039-9e77-a30ac8ab4627","executionInfo":{"status":"ok","timestamp":1567225062943,"user_tz":-540,"elapsed":117975,"user":{"displayName":"Bartłomiej Cerek","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBCye5yJ9tW1vLbOr7P95dVuwBX69OragQhxO_BMQ=s64","userId":"03865172822680659937"}},"colab":{"base_uri":"https://localhost:8080/","height":255}},"source":["#Get random flagged sample  \n","r = np.random.choice(f_indices)\n","#X is from preprocessing step of creating dataset. It's tensor of normalized data.\n","exp_sample = X[r]\n","if gpu:\n","    exp_sample = exp_sample.cuda()\n","    \n","model.eval()\n","output = model.forward(exp_sample)\n","loss = criterion(output, exp_sample)\n","\n","print('MSE value for sample: %.7f with boundary %.7f' % (loss, class_boundary))\n","\n","if loss >= class_boundary: \n","    print('This sample is classified as FLAGGED.')\n","else:\n","    print('This sample is classified as NOT FLAGGED.')\n","\n","#Get features that differ the most\n","feat_order = pd.DataFrame(columns=['Col','Feat_diff'])\n","feat_order['Col'] = filtered.columns\n","feat_order['Feat_diff'] = abs(exp_sample.cpu().data.numpy() - output.cpu().data.numpy())\n","feat_order = feat_order.sort_values('Feat_diff',ascending = False)\n","\n","print('Features that differ the most from expected values are:')\n","feat_order.head()"],"execution_count":69,"outputs":[{"output_type":"stream","text":["MSE value for sample: 0.0021189 with boundary 0.0192579\n","This sample is classified as NOT FLAGGED.\n","Features that differ the most from expected values are:\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Col</th>\n","      <th>Feat_diff</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>31</th>\n","      <td>slopedZAPos</td>\n","      <td>0.205360</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>offsetdZCPos</td>\n","      <td>0.178952</td>\n","    </tr>\n","    <tr>\n","      <th>134</th>\n","      <td>deltaPtchi2A</td>\n","      <td>0.168511</td>\n","    </tr>\n","    <tr>\n","      <th>212</th>\n","      <td>phiPullHighPt</td>\n","      <td>0.163224</td>\n","    </tr>\n","    <tr>\n","      <th>177</th>\n","      <td>dcaz_negA_2</td>\n","      <td>0.144019</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["               Col  Feat_diff\n","31     slopedZAPos   0.205360\n","32    offsetdZCPos   0.178952\n","134   deltaPtchi2A   0.168511\n","212  phiPullHighPt   0.163224\n","177    dcaz_negA_2   0.144019"]},"metadata":{"tags":[]},"execution_count":69}]},{"cell_type":"markdown","metadata":{"id":"zqFzlu3_1wDo","colab_type":"text"},"source":["##Result file example"]},{"cell_type":"code","metadata":{"id":"eibk7LHM1y8h","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"daadace3-0857-493e-b87f-8854631f4f0d","executionInfo":{"status":"ok","timestamp":1567225063267,"user_tz":-540,"elapsed":118292,"user":{"displayName":"Bartłomiej Cerek","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBCye5yJ9tW1vLbOr7P95dVuwBX69OragQhxO_BMQ=s64","userId":"03865172822680659937"}}},"source":["#Run model on all data\n","X_r = X\n","X_r = X_r.cuda()\n","output_r = model.forward(X_r)\n","\n","print(X[0])\n","print(output_r[0])\n","\n","loss_func = nn.MSELoss()\n","losses = []\n","for d,o in zip(X_r, output_r):\n","    losses.append(loss_func(d,o).item())\n","\n","result = pd.DataFrame()\n","result['period'] = period \n","result['run'] = run \n","result['chunkID'] = chunkID\n","result['scores'] = losses\n","\n","result.to_csv('SimpleAE_result.csv')\n","result.head()"],"execution_count":70,"outputs":[{"output_type":"stream","text":["tensor([-1.0000,  0.9842,  0.8049,  0.9265,  0.7847, -0.1414,  0.0416,  0.0452,\n","         0.0416,  0.9816,  0.7703, -0.0091, -0.0051,  0.0393, -0.0051,  1.0000,\n","         0.1735,  0.2586,  0.0435,  0.2755, -0.2337,  0.0850,  0.0372,  0.0323,\n","         0.0123,  0.0145,  0.0673,  0.0673,  0.1271,  0.1271,  0.0607,  0.3115,\n","        -0.2507,  0.1003,  0.0110,  0.0190,  0.0060,  0.0069,  0.0203,  0.0203,\n","         0.1041,  0.1041,  0.0252,  0.0837, -0.1989,  0.0448,  0.0141,  0.0161,\n","         0.0092,  0.0127,  0.1365,  0.1365,  0.1546,  0.1546,  0.0808,  0.3255,\n","        -0.6843, -0.5355,  0.0296,  0.0339,  0.0000,  0.0000,  0.0417,  0.0417,\n","         0.1655,  0.1655,  0.0451,  0.1360,  0.0571,  0.1581,  0.1894, -0.1717,\n","        -0.2992, -0.1628,  0.0322,  0.0277,  0.0000,  0.0000,  0.0178,  0.0178,\n","         0.1669,  0.1669, -0.0464,  0.6789, -0.3537, -0.3717,  0.0114,  0.0150,\n","         0.0000,  0.0000,  0.2052,  0.2052,  0.1708,  0.1708,  0.0726,  0.9228,\n","         0.7423,  0.0726,  0.9810,  0.4332,  0.0726,  0.1489,  0.8714,  0.9682,\n","         0.0723,  0.0726,  0.0726,  0.0726,  0.4403,  0.6827,  0.4353,  0.6760,\n","         0.0345,  0.4435,  0.6713,  0.0368,  0.4632,  0.8918,  0.7612,  0.4724,\n","         0.8786,  0.5788,  0.4426,  0.8751,  0.8202,  0.4457,  0.8886,  0.7851,\n","         0.0000,  0.0000,  0.0000, -0.0043,  0.2295,  0.0018,  0.2684, -0.0079,\n","         0.2340,  0.0350,  0.0233,  0.0151,  0.1595, -0.4367,  0.1837,  0.1383,\n","         0.0871,  0.0917,  0.0850,  0.2264, -0.1078, -0.0045,  0.1249,  0.0790,\n","         0.0767,  0.0808, -0.2075,  0.0275,  0.0738,  0.1057,  0.0780,  0.0810,\n","         0.0823, -0.2429, -0.1117, -0.3433,  0.1771,  0.0846,  0.0865,  0.0835,\n","         0.3080, -0.0427,  0.1549,  0.0618,  0.0891,  0.0821,  0.0912,  0.0847,\n","        -0.0894,  0.0342,  0.1308,  0.0752,  0.0731,  0.0769, -0.5280,  0.1345,\n","        -0.1482,  0.1391,  0.0824,  0.0802,  0.0844,  0.1608, -0.1095,  0.0035,\n","         0.1155,  0.0926,  0.0896,  0.0960,  1.0000,  1.0000,  1.0000,  1.0000,\n","         0.0066, -0.0019,  0.5576,  0.8233,  0.3394,  0.0571,  0.1647,  0.7509,\n","         0.7403,  0.7507,  0.7032, -0.0267, -0.0233,  0.7632,  0.4869, -0.1078,\n","        -0.0990, -0.5177, -0.3967,  0.5363,  0.0394,  0.2080, -0.1944,  0.2529,\n","         0.2527,  0.0301,  0.2628,  0.2625,  0.0296,  0.9685,  0.9684,  0.0033,\n","         0.9685,  0.9684,  0.0305, -0.9498, -0.9498,  0.0331,  0.2361])\n","tensor([-9.7857e-01,  9.8934e-01,  8.1255e-01,  9.1898e-01,  7.8541e-01,\n","        -1.5502e-01,  8.0307e-02,  4.6740e-02,  8.0296e-02,  9.8569e-01,\n","         7.7224e-01, -1.9225e-02,  2.4787e-02,  4.0491e-02,  2.4771e-02,\n","         9.9699e-01,  4.4404e-01, -5.2602e-02,  3.3911e-02,  2.5486e-01,\n","         1.2148e-01,  3.4818e-01,  3.8840e-02,  3.2239e-02,  1.2661e-02,\n","         1.3095e-02,  1.1243e-01,  1.1242e-01,  1.8018e-01,  1.8018e-01,\n","         4.5060e-02,  3.0295e-01,  1.2851e-01,  3.0726e-01,  1.3240e-02,\n","         1.5097e-02,  9.3909e-03,  1.0440e-02,  1.1609e-01,  1.1616e-01,\n","         1.6440e-01,  1.6441e-01,  2.6319e-02,  6.4747e-02,  1.0628e-01,\n","         3.0267e-01,  1.3525e-02,  1.3355e-02,  1.2485e-02,  1.3840e-02,\n","         9.9083e-02,  9.9131e-02,  1.9676e-01,  1.9676e-01,  5.1006e-02,\n","         3.1564e-01, -5.1292e-01, -4.3512e-01,  2.8874e-02,  3.4497e-02,\n","         8.6898e-05,  1.0318e-04,  8.3703e-02,  8.3732e-02,  2.1700e-01,\n","         2.1700e-01,  4.5077e-02,  1.3498e-01,  5.8674e-02,  1.5797e-01,\n","         1.3111e-01, -1.6739e-01, -1.9407e-01, -6.1557e-02,  3.2068e-02,\n","         2.5789e-02, -7.6886e-05,  3.3671e-04,  2.0907e-02,  2.0910e-02,\n","         2.1116e-01,  2.1117e-01, -3.7865e-02,  6.3808e-01, -2.8508e-01,\n","        -3.3461e-01,  1.3258e-02,  1.3785e-02, -1.1390e-04, -3.0074e-04,\n","         3.2722e-01,  3.2719e-01,  2.3877e-01,  2.3877e-01,  7.9454e-02,\n","         9.2029e-01,  7.3450e-01,  7.9377e-02,  9.8371e-01,  4.2446e-01,\n","         7.9444e-02, -7.7570e-03,  8.5951e-01,  9.7096e-01,  7.9117e-02,\n","         7.9414e-02,  7.9412e-02,  7.9390e-02,  4.3213e-01,  6.8254e-01,\n","         4.2641e-01,  6.7564e-01,  3.5701e-02,  4.3626e-01,  6.7186e-01,\n","         3.8764e-02,  4.6255e-01,  8.9352e-01,  7.7198e-01,  4.7147e-01,\n","         8.7961e-01,  5.8561e-01,  4.4286e-01,  8.7841e-01,  8.2008e-01,\n","         4.4583e-01,  8.9001e-01,  7.7622e-01, -3.3312e-02,  2.5088e-03,\n","         7.7425e-03, -2.3915e-02,  2.5989e-01, -2.4607e-02,  2.6297e-01,\n","        -9.4742e-03,  2.9524e-01,  3.6844e-02,  2.0532e-02,  1.4883e-02,\n","         9.5217e-02, -3.8667e-01, -4.6225e-02,  1.0947e-01,  8.6225e-02,\n","         9.0000e-02,  8.4345e-02,  2.2046e-01, -9.3113e-02, -2.1027e-02,\n","         1.1747e-01,  7.8736e-02,  7.6720e-02,  8.0168e-02, -1.0078e-01,\n","         2.5686e-02,  4.0985e-02,  1.2269e-01,  7.8380e-02,  8.0571e-02,\n","         8.1861e-02, -1.1837e-01, -1.3093e-01, -2.9153e-01,  1.6481e-01,\n","         8.4445e-02,  8.5728e-02,  8.3903e-02,  3.1088e-01, -2.9834e-02,\n","         1.2610e-01,  5.5309e-02,  8.7309e-02,  8.1757e-02,  8.8993e-02,\n","         7.2302e-02, -2.2088e-02, -1.4270e-02,  1.1567e-01,  7.5814e-02,\n","         7.4184e-02,  7.7129e-02, -2.0609e-01,  7.7711e-02, -1.5244e-01,\n","         1.7352e-01,  8.1720e-02,  7.9683e-02,  8.3314e-02,  2.1266e-01,\n","        -1.3483e-01,  5.5307e-03,  1.1324e-01,  8.9953e-02,  8.7454e-02,\n","         9.2577e-02,  9.9841e-01,  9.9780e-01,  9.9186e-01,  9.9217e-01,\n","        -8.1927e-03, -1.3725e-03,  5.5176e-01,  8.2459e-01,  3.3987e-01,\n","         5.7607e-02,  1.6452e-01,  7.5423e-01,  7.4582e-01,  7.5077e-01,\n","         7.0372e-01, -1.6895e-02,  1.1707e-02,  7.7273e-01,  5.1488e-01,\n","        -6.9249e-02, -3.7969e-02, -5.0310e-01, -4.7871e-01,  5.4473e-01,\n","         4.9125e-02,  1.5001e-01, -1.0451e-01,  2.6328e-01,  2.6339e-01,\n","         7.1312e-02,  2.7208e-01,  2.7218e-01,  7.1367e-02,  9.7793e-01,\n","         9.7732e-01,  1.2142e-02,  9.7557e-01,  9.7687e-01,  7.3408e-02,\n","        -8.6326e-01, -8.6341e-01,  7.3422e-02,  2.3376e-01], device='cuda:0',\n","       grad_fn=<SelectBackward>)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>period</th>\n","      <th>run</th>\n","      <th>chunkID</th>\n","      <th>scores</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>LHC18f</td>\n","      <td>287000</td>\n","      <td>1</td>\n","      <td>0.004933</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>LHC18f</td>\n","      <td>287000</td>\n","      <td>2</td>\n","      <td>0.001733</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>LHC18f</td>\n","      <td>287000</td>\n","      <td>3</td>\n","      <td>0.002753</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>LHC18f</td>\n","      <td>287000</td>\n","      <td>4</td>\n","      <td>0.006438</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>LHC18f</td>\n","      <td>287000</td>\n","      <td>5</td>\n","      <td>0.002859</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   period     run  chunkID    scores\n","0  LHC18f  287000        1  0.004933\n","1  LHC18f  287000        2  0.001733\n","2  LHC18f  287000        3  0.002753\n","3  LHC18f  287000        4  0.006438\n","4  LHC18f  287000        5  0.002859"]},"metadata":{"tags":[]},"execution_count":70}]},{"cell_type":"code","metadata":{"id":"E0YzSmZm3ska","colab_type":"code","colab":{}},"source":["#Count sum loss and aeScore from data \n","def getScores(scores, labels, scale):\n","    if scale: #If scale scale scores 0-1\n","        min_max_scaler = preprocessing.MinMaxScaler()\n","        scores = min_max_scaler.fit_transform(np.array(scores).reshape(-1,1)).squeeze()\n","\n","    tot_loss = 0\n","    outliers = []\n","    corr = []\n","    for s,l in zip(scores,labels):\n","        tot_loss += s\n","        if l:\n","            outliers.append(s)\n","        else:\n","            corr.append(s)\n","        \n","    loss = tot_loss / len(labels)\n","    asScore = sum(outliers)/len(outliers)  -  sum(corr)/len(corr)\n","    return loss,asScore "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZV6Ju6cx3tSX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"916b8213-6f63-4aec-915c-ed065db0ade1","executionInfo":{"status":"ok","timestamp":1567225063269,"user_tz":-540,"elapsed":118282,"user":{"displayName":"Bartłomiej Cerek","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBCye5yJ9tW1vLbOr7P95dVuwBX69OragQhxO_BMQ=s64","userId":"03865172822680659937"}}},"source":["loss, asScore = getScores(losses,raw[war_col],True)\n","\n","print('Total loss: {:.2f}'.format(loss*1000))\n","print('AutoEncoder Score: {:.2f}'.format(asScore*1000))"],"execution_count":72,"outputs":[{"output_type":"stream","text":["Total loss: 16.00\n","AutoEncoder Score: 121.18\n"],"name":"stdout"}]}]}