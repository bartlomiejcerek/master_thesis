{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BasicAutoencoders.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"LCQBk-cXMR1R","colab_type":"text"},"source":["# Simple Autoencoder Function\n","\n","## 1. Import modules and set paths\n","\n","All used libraries are here."]},{"cell_type":"code","metadata":{"id":"c4hXmiQRMEBt","colab_type":"code","colab":{}},"source":["# Data managment \n","import pandas as pd \n","import numpy as np \n","\n","#Machine learning\n","import torch as t\n","from torch import nn, optim\n","from torch.utils import data as data_lib\n","import torch.nn.functional as F\n","\n","#Utilities\n","import time\n","import os"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-01U0zg-aIJ0","colab_type":"text"},"source":["## 2. Preprocess and create generators.\n","Generators will be used for training, validation and test.\n","\n","Some specific features of PyTorch are used."]},{"cell_type":"code","metadata":{"id":"EvBT55eubIDk","colab_type":"code","colab":{}},"source":["def prepareData(data):\n","  \n","  no_samples, no_features = data.shape\n","  no_features = no_features - 1 #Take -1 becasue of label column\n","  \n","  labels = data['label']\n","  data = data.drop('label', axis = 1)\n","  \n","  crit = labels.astype(bool)\n","  f_indices = np.where(crit)[0] # Flagged indices\n","  no_f_indices = np.where(~crit)[0] # Not flagged indices\n","\n","  #Shuffle intances to ensure that with each run different samples are drawn\n","  np.random.seed(int(time.time()))\n","  np.random.shuffle(f_indices)\n","  np.random.shuffle(no_f_indices)\n","\n","  # 10% of all correct instances will be used as validation\n","  valid_split = int(np.floor(.1 * len(no_f_indices))) \n","  # 90% of all samples that were calssified as outliers will be used for test \n","  # since none of them are used for traning. 10% are rejected to not overfit.\n","  # Same amount correct and flagged samples will be used as test.\n","  test_split = int(np.floor(.9* sum(crit)))\n","\n","  #Create dataset\n","  X = t.FloatTensor(data.to_numpy()) #features\n","  Y = t.IntTensor(labels.to_numpy()) #labels\n","  dataset = data_lib.TensorDataset(X,Y)\n","\n","  #Set training indices of correct cases\n","  test_indices_c, valid_indices, train_indices = np.split(no_f_indices, [test_split, test_split+valid_split])\n","  # Add flagged indices to test indices \n","  test_indices = np.concatenate((test_indices_c, f_indices[:test_split]))\n","\n","  # Create samplers\n","  train_sampler = data_lib.SubsetRandomSampler(train_indices)\n","  valid_sampler = data_lib.SubsetRandomSampler(valid_indices)\n","  test_sampler = data_lib.SubsetRandomSampler(test_indices)\n","\n","  #Parameters\n","  params = {'batch_size': 10, 'num_workers': 8}\n","\n","  #Create generators\n","  train_gen = data_lib.DataLoader(dataset, **params,sampler=train_sampler)\n","  valid_gen = data_lib.DataLoader(dataset, **params, sampler=valid_sampler)\n","  test_gen = data_lib.DataLoader(dataset, **params, sampler=test_sampler)\n","  \n","  return (no_samples, no_features, train_gen, valid_gen, test_gen)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uB0TxZ2kueYX","colab_type":"text"},"source":["## 3. Definine architectures of Neural Networks\n","For now implemented:\n","\n","-Simple Autoencoder"]},{"cell_type":"markdown","metadata":{"id":"kM7GOwwqddXT","colab_type":"text"},"source":["### Simple Autoencoder"]},{"cell_type":"code","metadata":{"id":"j0_NAoyVdYTv","colab_type":"code","colab":{}},"source":["class Autoencoder(nn.Module):\n","    def __init__(self, no_features):\n","        super().__init__()\n","        NF = no_features\n","        \n","        self.fc1 = nn.Linear(NF, round(0.9*NF))\n","        self.fc2 = nn.Linear(round(0.9*NF), round(0.8*NF))\n","        \n","        self.fc3 = nn.Linear(round(0.8*NF), round(0.7*NF)) \n","        self.fc4 = nn.Linear(round(0.7*NF), round(0.8*NF))\n","        \n","        self.fc5 = nn.Linear(round(0.8*NF), round(0.9*NF))\n","        self.fc6 = nn.Linear(round(0.9*NF), NF)  \n","        \n","        self.dropout = nn.Dropout(p=0.25)\n","        \n","    def forward(self, x):\n","        \n","        x = self.dropout(F.relu(self.fc1(x)))\n","        x = self.dropout(F.relu(self.fc2(x)))\n","        x = self.dropout(F.relu(self.fc3(x)))\n","        x = self.dropout(F.relu(self.fc4(x)))\n","        x = self.dropout(F.relu(self.fc5(x)))\n","                               \n","        x = F.relu(self.fc6(x))\n","        return x"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SNlMUiVhVh2x","colab_type":"text"},"source":["## 4. Train the network function\n","Network is being trained only on **not flagged** data!"]},{"cell_type":"code","metadata":{"id":"-Fw1QlN8VjS0","colab_type":"code","colab":{}},"source":["def trainModel(n_epochs, model, train_gen, valid_gen, model_path, gpu, optimizer, criterion):\n","\n","  valid_loss_min = np.Inf # to track change in validation loss\n","\n","  # Iterate on epochs \n","  for epoch in range(1, n_epochs+1):\n","\n","      # keep track of training and validation loss\n","      train_loss = 0.0\n","      valid_loss = 0.0\n","\n","      # Set model to train mode (to include dropout)\n","      model.train()\n","\n","      # iterate on data batches, discard labels\n","      for features, _ in train_gen:\n","          # move tensors to GPU if CUDA is available\n","          if gpu:\n","              features = features.cuda()\n","          # clear the gradients of all optimized variables\n","          optimizer.zero_grad()\n","          # forward pass: compute predicted outputs by passing inputs throught the model\n","          output = model.forward(features)\n","          # calculate the batch loss by compering to initial features\n","          loss = criterion(output, features)\n","          # backward pass: compute gradient of the loss with respect to model parameters\n","          loss.backward()\n","          # perform a single optimization step (parameter update)\n","          optimizer.step()\n","          # update average validation loss, mulitply by batchsize for bigger nums\n","          train_loss += loss.item() *features.size(0)\n","\n","      # Validate the model \n","\n","      # Set model to evaluation mode tu use its full power\n","      model.eval()\n","\n","      for features, _ in valid_gen:\n","          # move tensors to GPU if CUDA is available\n","          if gpu:\n","              features = features.cuda()\n","          # forward pass: compute predicted outputs by passing inputs to the model\n","          output = model.forward(features)\n","          # calculate the batch loss by compering to initial features\n","          loss = criterion(output, features)\n","          # update average validation loss, mulitply by batchsize for bigger nums\n","          valid_loss += loss.item() *features.size(0)\n","\n","      # calculate average losses\n","      train_loss = train_loss/len(train_gen.sampler)\n","      valid_loss = valid_loss/len(valid_gen.sampler)\n","\n","      # save model if validation loss has decreased\n","      if valid_loss <= valid_loss_min:\n","          t.save(model.state_dict(), model_path)\n","          valid_loss_min = valid_loss"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s8sIc-roy0ix","colab_type":"text"},"source":["## 5. Test\n","Load model which had smallest validation loss in training. \n","\n","**Count losses**\n","\n","Calcualte sum of losses sepratly for **outliers** and **correct samples** and divide them by half of sampler length. Amount of samples in both cathegories is same. This way we obtain avg MSE for both types.\n","\n"]},{"cell_type":"code","metadata":{"id":"2rqpNUNwXn8j","colab_type":"code","colab":{}},"source":["def testModel(test_gen, model, model_path, gpu, criterion):\n","\n","  #Load model with best parameters\n","  model.load_state_dict(t.load(model_path))\n","\n","  # track test loss\n","  outliers_loss = 0.0\n","  correct_loss = 0.0\n","  loss_lab_list = []\n","\n","  # Set model to evaluation to use its full power\n","  model.eval()\n","  # iterate over test data\n","  for features, labels in test_gen:\n","      # move tensors to GPU if CUDA is available\n","      if gpu:\n","         features, labels = features.cuda(), labels.cuda()\n","      # forward pass: compute predicted outputs by passing inputs to the model\n","      output = model.forward(features)\n","\n","      # Update test loss for score purposes\n","      for samp_no, lab in enumerate(labels):\n","          loss = criterion(output[samp_no], features[samp_no])\n","          # Create list of losses and labels for next step\n","          loss_lab_list.append((loss.item(), lab.item()))\n","          if lab.item(): # true if flagged\n","              outliers_loss += loss.item()\n","          else:\n","              correct_loss += loss.item()\n","\n","  # Print average MSE\n","  avg_outlieres_loss = outliers_loss / (len(test_gen.sampler)/2)\n","  avg_correct_loss = correct_loss / (len(test_gen.sampler)/2)\n","  return avg_outlieres_loss, avg_correct_loss"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"56uSk-ItDI68","colab_type":"text"},"source":["## 6. Main functions"]},{"cell_type":"markdown","metadata":{"id":"5Q8TY3FNdhoJ","colab_type":"text"},"source":["###Simple Autoencoder"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"iwZrCoqwS5uN","colab":{}},"source":["def getSimpleAutoencoderLosses(data, no_epochs, model_path):\n","    \"\"\"This function trains autoencoder on given data.\n","    \n","    It will also test model and return obtained test scores.\n","    \n","    Returns:\n","    \n","    float - avg_outlieres_loss - Average MSE of test samples with outlier label   \n","    float - avg_correct_loss - Average MSE of test samples without label  \n","    \"\"\"\n","    no_samples, no_features, train_gen, valid_gen, test_gen = prepareData(data)\n","    # Put net to model object\n","    model = Autoencoder(no_features)\n","    # Mean Square Error criterion\n","    criterion = nn.MSELoss()\n","    # Optimizer - Adam\n","    optimizer = optim.Adam(model.parameters(), lr=0.003)\n","\n","    gpu = t.cuda.is_available()\n","\n","    #Set hardware variable (to know if moving model to gpu is option)\n","    if not gpu:\n","        model.cpu()\n","    else:\n","        model.cuda()\n","\n","    trainModel(no_epochs, model, train_gen, valid_gen, model_path, gpu, optimizer, criterion)\n","    avg_outlieres_loss, avg_correct_loss = testModel(test_gen, model, model_path, gpu, criterion)  \n","\n","    return avg_outlieres_loss, avg_correct_loss"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QJrEDfCsxByq","colab_type":"text"},"source":["## 7. Test \n","Should be commented before notebook is used."]},{"cell_type":"code","metadata":{"id":"KR5dml_R5ni9","colab_type":"code","colab":{}},"source":["# %%capture\n","# #Get data, small file so no if\n","# if not os.path.isfile('all_scaled0_1.csv'): \n","#     !wget 'https://drive.google.com/uc?export=download&id=1-ET9vXPKudU92XuWeR0wIL67byS2llq-' -O all_scaled0_1.csv"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iDzti_5kEAjM","colab_type":"code","outputId":"60aa9733-9983-4fd4-9e71-476e04f0c5e6","executionInfo":{"status":"ok","timestamp":1570212469958,"user_tz":-120,"elapsed":25323,"user":{"displayName":"BartÅ‚omiej Cerek","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBCye5yJ9tW1vLbOr7P95dVuwBX69OragQhxO_BMQ=s64","userId":"03865172822680659937"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["# no_epochs = 10\n","# model_path = 'simple_ae_model.pt'\n","# data = pd.read_csv('all_scaled0_1.csv',index_col=0)\n","# data = data.drop(['chunkID','run','period'], axis = 1)\n","  \n","# avg_outlieres_loss, avg_correct_loss = getSimpleAutoencoderLosses(data, no_epochs, model_path)\n","\n","# print(avg_outlieres_loss*1000)\n","# print(avg_correct_loss*1000)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["41.66836613168319\n","3.538534586162617\n","\n","Score:\n","38.129831545520574\n"],"name":"stdout"}]}]}