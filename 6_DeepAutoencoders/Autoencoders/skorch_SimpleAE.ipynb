{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"skorch_SimpleAE.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"R-hdYpTwKJNI","colab_type":"text"},"source":["#Basic Autoencoder"]},{"cell_type":"markdown","metadata":{"id":"O_lU8-wuKSxy","colab_type":"text"},"source":["##Import libraries and get data"]},{"cell_type":"code","metadata":{"id":"rKGHDulZqmjC","colab_type":"code","colab":{}},"source":["%%capture\n","#Install skorch package \n","!pip install skorch"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1Ept7zHwJVaI","colab_type":"code","colab":{}},"source":["#Data managment \n","import pandas as pd \n","import numpy as np \n","\n","#Machine learning\n","import torch as t\n","from torch import nn, optim\n","import torch.nn.functional as F\n","\n","#Sklearn tools\n","from sklearn.model_selection import train_test_split\n","\n","#Import model related libraries\n","import skorch\n","from skorch import NeuralNetRegressor\n","from skorch.callbacks import Checkpoint, LRScheduler\n","\n","#Utilities\n","import os"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"K8S3Nr66KPBS","colab_type":"code","colab":{}},"source":["%%capture\n","#Get data\n","if not os.path.isfile('all_scaled0_1.csv'): \n","    !wget 'https://drive.google.com/uc?export=download&id=1-ET9vXPKudU92XuWeR0wIL67byS2llq-' -O all_scaled0_1.csv\n","if not os.path.isfile('data_final_feat_scaled0_1.csv'): \n","    !wget 'https://drive.google.com/uc?export=download&id=14Qasv71GRD3kT3GhyI2v6c8v4L6YIBy0' -O data_final_feat_scaled0_1.csv"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"13KQ34ymLNtm","colab_type":"text"},"source":["##Parse Data"]},{"cell_type":"code","metadata":{"id":"o3GjB1AqbZrk","colab_type":"code","colab":{}},"source":["#data_file = 'all_scaled0_1.csv'\n","data_file = 'data_final_feat_scaled0_1.csv'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3-GjTy6pLRrD","colab_type":"code","outputId":"cdf39b3a-a1fd-40e1-ec84-f9a4939d6691","executionInfo":{"status":"ok","timestamp":1574347985737,"user_tz":-60,"elapsed":17011,"user":{"displayName":"Bart≈Çomiej Cerek","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDlsR1yZSDN4MjBT0j3iDjqMfyCHSbnh0KNKZ9Q5rA=s64","userId":"03865172822680659937"}},"colab":{"base_uri":"https://localhost:8080/","height":321}},"source":["data = pd.read_csv(data_file,index_col=0)\n","labels = data['label']\n","run = data['run']\n","chunkID = data['chunkID']\n","period = data['period']\n","data = data.drop(['label','chunkID','run','period'], axis = 1)\n","no_samples, no_features = data.shape\n","print(no_samples, no_features)\n","print(labels.value_counts())\n","data.head()"],"execution_count":5,"outputs":[{"output_type":"stream","text":["3429 128\n","0.0    3262\n","1.0     167\n","Name: label, dtype: int64\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>dcar_posA_2_Err</th>\n","      <th>slopedZAchi2Pos</th>\n","      <th>MIPattachSlopeC</th>\n","      <th>meanMultNeg</th>\n","      <th>mediumPtAPos</th>\n","      <th>dcarCP0</th>\n","      <th>dcaz_negA_1_Err</th>\n","      <th>tpcConstrainPhiA</th>\n","      <th>dcar_negA_chi2</th>\n","      <th>offsetdZC</th>\n","      <th>dcaz_negA_0_Err</th>\n","      <th>zPullHighPt</th>\n","      <th>vertOK</th>\n","      <th>tpcItsMatchHighPtA</th>\n","      <th>deltaPt</th>\n","      <th>resolutionMIP</th>\n","      <th>entriesVertY</th>\n","      <th>entriesMult</th>\n","      <th>dcaz_posA_0</th>\n","      <th>dcar_posC_1</th>\n","      <th>slopedRAchi2Pos</th>\n","      <th>tpcConstrainPhiC</th>\n","      <th>iroc_A_side</th>\n","      <th>meanVertZ</th>\n","      <th>slopedZAchi2</th>\n","      <th>deltaPtC</th>\n","      <th>offsetdZAErr</th>\n","      <th>offsetdZA</th>\n","      <th>dcaz_posA_2_Err</th>\n","      <th>dcaz_negC_0_Err</th>\n","      <th>offsetdRA</th>\n","      <th>meanMIPele</th>\n","      <th>offsetdZAchi2Pos</th>\n","      <th>medianHVandPTGainCorrIROC</th>\n","      <th>iroc_C_side</th>\n","      <th>vertAll</th>\n","      <th>dcar_negA_1_Err</th>\n","      <th>offsetdRAchi2Pos</th>\n","      <th>meanMult</th>\n","      <th>qOverPtC</th>\n","      <th>...</th>\n","      <th>phiPull</th>\n","      <th>rmsTPCChi2</th>\n","      <th>dcar_negC_1_Err</th>\n","      <th>offsetdRCchi2</th>\n","      <th>meanTPCChi2</th>\n","      <th>rmsVertY</th>\n","      <th>tpcItsMatchHighPtC</th>\n","      <th>dcaz_negC_2_Err</th>\n","      <th>yPullHighPt</th>\n","      <th>tpcItsMatchC</th>\n","      <th>deltaPtchi2C</th>\n","      <th>deltaPtchi2</th>\n","      <th>rmsMultNeg</th>\n","      <th>meanTPCnclF</th>\n","      <th>ptPullHighPt</th>\n","      <th>oroc_C_side</th>\n","      <th>slopedZCchi2Pos</th>\n","      <th>rmsTPCncl</th>\n","      <th>dcar_posC_2_Err</th>\n","      <th>offsetdZCchi2Pos</th>\n","      <th>deltaPtchi2A</th>\n","      <th>resolutionMIPele</th>\n","      <th>dcarCP1</th>\n","      <th>dcar_posA_1_Err</th>\n","      <th>slopedRCchi2Pos</th>\n","      <th>slopeCTPCnclFErr</th>\n","      <th>slopeCTPCnclF</th>\n","      <th>dcar_negC_0_Err</th>\n","      <th>slopedZC</th>\n","      <th>lambdaPull</th>\n","      <th>dcaz_posC_1_Err</th>\n","      <th>offsetdRCchi2Neg</th>\n","      <th>dcaz_negC_chi2</th>\n","      <th>offsetdRCchi2Pos</th>\n","      <th>dcaz_posC_0_Err</th>\n","      <th>oroc_A_side</th>\n","      <th>offsetdZAchi2</th>\n","      <th>dcar_negA_0_Err</th>\n","      <th>dcar_negC_chi2</th>\n","      <th>slopeATPCnclF</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.084978</td>\n","      <td>0.018955</td>\n","      <td>0.842762</td>\n","      <td>0.171907</td>\n","      <td>0.269421</td>\n","      <td>0.012700</td>\n","      <td>0.073086</td>\n","      <td>0.365811</td>\n","      <td>0.040081</td>\n","      <td>0.456515</td>\n","      <td>0.075152</td>\n","      <td>0.494765</td>\n","      <td>0.072495</td>\n","      <td>0.573832</td>\n","      <td>0.778597</td>\n","      <td>0.094750</td>\n","      <td>0.072495</td>\n","      <td>0.072495</td>\n","      <td>0.402053</td>\n","      <td>0.832478</td>\n","      <td>0.017025</td>\n","      <td>0.029029</td>\n","      <td>1.0</td>\n","      <td>0.314548</td>\n","      <td>0.066360</td>\n","      <td>0.607326</td>\n","      <td>0.029047</td>\n","      <td>0.676500</td>\n","      <td>0.080796</td>\n","      <td>0.064750</td>\n","      <td>0.827629</td>\n","      <td>0.539362</td>\n","      <td>0.018955</td>\n","      <td>0.643058</td>\n","      <td>1.0</td>\n","      <td>0.072211</td>\n","      <td>0.082068</td>\n","      <td>0.017025</td>\n","      <td>0.170502</td>\n","      <td>0.999787</td>\n","      <td>...</td>\n","      <td>0.279488</td>\n","      <td>0.299807</td>\n","      <td>0.089595</td>\n","      <td>0.165264</td>\n","      <td>0.586648</td>\n","      <td>0.111019</td>\n","      <td>0.582128</td>\n","      <td>0.066403</td>\n","      <td>0.397318</td>\n","      <td>0.586826</td>\n","      <td>0.232100</td>\n","      <td>0.210562</td>\n","      <td>0.220209</td>\n","      <td>0.650378</td>\n","      <td>0.478282</td>\n","      <td>1.0</td>\n","      <td>0.103951</td>\n","      <td>0.130696</td>\n","      <td>0.062340</td>\n","      <td>0.103951</td>\n","      <td>0.254022</td>\n","      <td>0.377138</td>\n","      <td>0.863747</td>\n","      <td>0.091667</td>\n","      <td>0.166519</td>\n","      <td>0.506033</td>\n","      <td>0.506033</td>\n","      <td>0.092639</td>\n","      <td>0.479150</td>\n","      <td>0.564013</td>\n","      <td>0.063305</td>\n","      <td>0.169389</td>\n","      <td>0.049049</td>\n","      <td>0.166519</td>\n","      <td>0.060923</td>\n","      <td>1.0</td>\n","      <td>0.066360</td>\n","      <td>0.089056</td>\n","      <td>0.096599</td>\n","      <td>0.567304</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.083858</td>\n","      <td>0.078152</td>\n","      <td>0.828729</td>\n","      <td>0.173973</td>\n","      <td>0.240232</td>\n","      <td>0.013681</td>\n","      <td>0.072295</td>\n","      <td>0.279536</td>\n","      <td>0.029575</td>\n","      <td>0.656468</td>\n","      <td>0.074312</td>\n","      <td>0.458420</td>\n","      <td>0.074775</td>\n","      <td>0.549569</td>\n","      <td>0.745472</td>\n","      <td>0.091434</td>\n","      <td>0.074775</td>\n","      <td>0.074775</td>\n","      <td>0.388408</td>\n","      <td>0.802137</td>\n","      <td>0.005406</td>\n","      <td>0.084570</td>\n","      <td>1.0</td>\n","      <td>0.298703</td>\n","      <td>0.098105</td>\n","      <td>0.597697</td>\n","      <td>0.028192</td>\n","      <td>0.672718</td>\n","      <td>0.079918</td>\n","      <td>0.062965</td>\n","      <td>0.794622</td>\n","      <td>0.539610</td>\n","      <td>0.078152</td>\n","      <td>0.644604</td>\n","      <td>1.0</td>\n","      <td>0.074384</td>\n","      <td>0.080961</td>\n","      <td>0.005406</td>\n","      <td>0.171713</td>\n","      <td>0.999787</td>\n","      <td>...</td>\n","      <td>0.295763</td>\n","      <td>0.299619</td>\n","      <td>0.087815</td>\n","      <td>0.277622</td>\n","      <td>0.579001</td>\n","      <td>0.115167</td>\n","      <td>0.581842</td>\n","      <td>0.064488</td>\n","      <td>0.428699</td>\n","      <td>0.583827</td>\n","      <td>0.149810</td>\n","      <td>0.385217</td>\n","      <td>0.236852</td>\n","      <td>0.652363</td>\n","      <td>0.508984</td>\n","      <td>1.0</td>\n","      <td>0.195836</td>\n","      <td>0.129801</td>\n","      <td>0.061326</td>\n","      <td>0.195836</td>\n","      <td>0.515967</td>\n","      <td>0.377678</td>\n","      <td>0.864576</td>\n","      <td>0.090594</td>\n","      <td>0.270753</td>\n","      <td>0.490807</td>\n","      <td>0.490807</td>\n","      <td>0.090666</td>\n","      <td>0.622429</td>\n","      <td>0.559882</td>\n","      <td>0.061817</td>\n","      <td>0.295496</td>\n","      <td>0.053690</td>\n","      <td>0.270753</td>\n","      <td>0.059619</td>\n","      <td>1.0</td>\n","      <td>0.098105</td>\n","      <td>0.088029</td>\n","      <td>0.080482</td>\n","      <td>0.553643</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.083916</td>\n","      <td>0.144460</td>\n","      <td>0.829774</td>\n","      <td>0.176758</td>\n","      <td>0.276467</td>\n","      <td>0.013235</td>\n","      <td>0.071540</td>\n","      <td>0.319029</td>\n","      <td>0.040652</td>\n","      <td>0.774050</td>\n","      <td>0.073681</td>\n","      <td>0.461882</td>\n","      <td>0.075070</td>\n","      <td>0.567858</td>\n","      <td>0.739555</td>\n","      <td>0.089755</td>\n","      <td>0.075070</td>\n","      <td>0.075069</td>\n","      <td>0.394434</td>\n","      <td>0.787007</td>\n","      <td>0.014864</td>\n","      <td>0.133775</td>\n","      <td>1.0</td>\n","      <td>0.281690</td>\n","      <td>0.102944</td>\n","      <td>0.589850</td>\n","      <td>0.027634</td>\n","      <td>0.709053</td>\n","      <td>0.079374</td>\n","      <td>0.062983</td>\n","      <td>0.800667</td>\n","      <td>0.539159</td>\n","      <td>0.144460</td>\n","      <td>0.644119</td>\n","      <td>1.0</td>\n","      <td>0.074775</td>\n","      <td>0.080976</td>\n","      <td>0.014864</td>\n","      <td>0.175191</td>\n","      <td>0.999787</td>\n","      <td>...</td>\n","      <td>0.281763</td>\n","      <td>0.302841</td>\n","      <td>0.087507</td>\n","      <td>0.349164</td>\n","      <td>0.582115</td>\n","      <td>0.119110</td>\n","      <td>0.586257</td>\n","      <td>0.064412</td>\n","      <td>0.420849</td>\n","      <td>0.584458</td>\n","      <td>0.421557</td>\n","      <td>0.435664</td>\n","      <td>0.234255</td>\n","      <td>0.656779</td>\n","      <td>0.493338</td>\n","      <td>1.0</td>\n","      <td>0.224404</td>\n","      <td>0.128520</td>\n","      <td>0.061049</td>\n","      <td>0.224404</td>\n","      <td>0.247578</td>\n","      <td>0.377654</td>\n","      <td>0.864001</td>\n","      <td>0.090476</td>\n","      <td>0.332814</td>\n","      <td>0.486826</td>\n","      <td>0.486826</td>\n","      <td>0.090231</td>\n","      <td>0.716557</td>\n","      <td>0.561373</td>\n","      <td>0.061571</td>\n","      <td>0.391960</td>\n","      <td>0.070414</td>\n","      <td>0.332814</td>\n","      <td>0.059297</td>\n","      <td>1.0</td>\n","      <td>0.102944</td>\n","      <td>0.088169</td>\n","      <td>0.067023</td>\n","      <td>0.576802</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.085152</td>\n","      <td>0.084640</td>\n","      <td>0.835028</td>\n","      <td>0.174482</td>\n","      <td>0.258437</td>\n","      <td>0.016565</td>\n","      <td>0.072717</td>\n","      <td>0.251844</td>\n","      <td>0.024053</td>\n","      <td>0.865839</td>\n","      <td>0.074933</td>\n","      <td>0.496001</td>\n","      <td>0.073988</td>\n","      <td>0.541670</td>\n","      <td>0.761264</td>\n","      <td>0.091761</td>\n","      <td>0.073988</td>\n","      <td>0.073987</td>\n","      <td>0.389906</td>\n","      <td>0.784608</td>\n","      <td>0.016920</td>\n","      <td>0.160170</td>\n","      <td>1.0</td>\n","      <td>0.291844</td>\n","      <td>0.074307</td>\n","      <td>0.600640</td>\n","      <td>0.029305</td>\n","      <td>0.684981</td>\n","      <td>0.080639</td>\n","      <td>0.064623</td>\n","      <td>0.779825</td>\n","      <td>0.539396</td>\n","      <td>0.084640</td>\n","      <td>0.637938</td>\n","      <td>1.0</td>\n","      <td>0.073704</td>\n","      <td>0.081358</td>\n","      <td>0.016920</td>\n","      <td>0.172629</td>\n","      <td>0.999787</td>\n","      <td>...</td>\n","      <td>0.270435</td>\n","      <td>0.307098</td>\n","      <td>0.088426</td>\n","      <td>0.427887</td>\n","      <td>0.590880</td>\n","      <td>0.113392</td>\n","      <td>0.589802</td>\n","      <td>0.066359</td>\n","      <td>0.439337</td>\n","      <td>0.572863</td>\n","      <td>0.565959</td>\n","      <td>0.406805</td>\n","      <td>0.229296</td>\n","      <td>0.650338</td>\n","      <td>0.537189</td>\n","      <td>1.0</td>\n","      <td>0.224069</td>\n","      <td>0.127972</td>\n","      <td>0.062087</td>\n","      <td>0.224069</td>\n","      <td>0.425016</td>\n","      <td>0.377537</td>\n","      <td>0.863877</td>\n","      <td>0.091592</td>\n","      <td>0.391857</td>\n","      <td>0.465535</td>\n","      <td>0.465535</td>\n","      <td>0.091349</td>\n","      <td>0.786867</td>\n","      <td>0.529956</td>\n","      <td>0.063318</td>\n","      <td>0.491013</td>\n","      <td>0.095369</td>\n","      <td>0.391857</td>\n","      <td>0.060874</td>\n","      <td>1.0</td>\n","      <td>0.074307</td>\n","      <td>0.088482</td>\n","      <td>0.065335</td>\n","      <td>0.573718</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.083390</td>\n","      <td>0.070673</td>\n","      <td>0.833388</td>\n","      <td>0.174964</td>\n","      <td>0.265868</td>\n","      <td>0.016429</td>\n","      <td>0.071677</td>\n","      <td>0.286252</td>\n","      <td>0.048721</td>\n","      <td>0.710244</td>\n","      <td>0.073648</td>\n","      <td>0.502829</td>\n","      <td>0.075331</td>\n","      <td>0.544351</td>\n","      <td>0.762631</td>\n","      <td>0.092249</td>\n","      <td>0.075331</td>\n","      <td>0.075331</td>\n","      <td>0.400363</td>\n","      <td>0.813738</td>\n","      <td>0.023178</td>\n","      <td>0.057441</td>\n","      <td>1.0</td>\n","      <td>0.322965</td>\n","      <td>0.073414</td>\n","      <td>0.601722</td>\n","      <td>0.027425</td>\n","      <td>0.666038</td>\n","      <td>0.079273</td>\n","      <td>0.063082</td>\n","      <td>0.793490</td>\n","      <td>0.539643</td>\n","      <td>0.070673</td>\n","      <td>0.637506</td>\n","      <td>1.0</td>\n","      <td>0.075002</td>\n","      <td>0.080794</td>\n","      <td>0.023178</td>\n","      <td>0.173473</td>\n","      <td>0.999787</td>\n","      <td>...</td>\n","      <td>0.292003</td>\n","      <td>0.319091</td>\n","      <td>0.087565</td>\n","      <td>0.390902</td>\n","      <td>0.591539</td>\n","      <td>0.116134</td>\n","      <td>0.561339</td>\n","      <td>0.064765</td>\n","      <td>0.408558</td>\n","      <td>0.586569</td>\n","      <td>0.200595</td>\n","      <td>0.274403</td>\n","      <td>0.223069</td>\n","      <td>0.654262</td>\n","      <td>0.515263</td>\n","      <td>1.0</td>\n","      <td>0.293339</td>\n","      <td>0.132655</td>\n","      <td>0.061176</td>\n","      <td>0.293339</td>\n","      <td>0.300306</td>\n","      <td>0.377276</td>\n","      <td>0.863615</td>\n","      <td>0.090399</td>\n","      <td>0.376772</td>\n","      <td>0.494138</td>\n","      <td>0.494138</td>\n","      <td>0.090079</td>\n","      <td>0.684728</td>\n","      <td>0.534762</td>\n","      <td>0.061603</td>\n","      <td>0.413305</td>\n","      <td>0.089652</td>\n","      <td>0.376772</td>\n","      <td>0.059297</td>\n","      <td>1.0</td>\n","      <td>0.073414</td>\n","      <td>0.087626</td>\n","      <td>0.076392</td>\n","      <td>0.584242</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows √ó 128 columns</p>\n","</div>"],"text/plain":["   dcar_posA_2_Err  slopedZAchi2Pos  ...  dcar_negC_chi2  slopeATPCnclF\n","0         0.084978         0.018955  ...        0.096599       0.567304\n","1         0.083858         0.078152  ...        0.080482       0.553643\n","2         0.083916         0.144460  ...        0.067023       0.576802\n","3         0.085152         0.084640  ...        0.065335       0.573718\n","4         0.083390         0.070673  ...        0.076392       0.584242\n","\n","[5 rows x 128 columns]"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"U8Xz7WE3QFbE","colab_type":"text"},"source":["##Split Data\n","If **train_on_all** is true labeled outliers are also used in training. If not then:\n","\n","**Test:** All outliers and same amount of randomly choosen correct samples\n","\n","**Training:** Rest of all correct samples"]},{"cell_type":"code","metadata":{"id":"6PfoWWNTZPL7","colab_type":"code","colab":{}},"source":["#Include outliers in training? \n","train_on_all = True\n","size_of_test = 0.2 #When traning only on correct samples it will be this % of corr and all outliers."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UeiitB3VRMY4","colab_type":"code","outputId":"ca3dd48d-c4c1-4c02-deb7-d0484e159176","executionInfo":{"status":"ok","timestamp":1574347985739,"user_tz":-60,"elapsed":17003,"user":{"displayName":"Bart≈Çomiej Cerek","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDlsR1yZSDN4MjBT0j3iDjqMfyCHSbnh0KNKZ9Q5rA=s64","userId":"03865172822680659937"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["if train_on_all:\n","    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=size_of_test,stratify=labels)\n","else:\n","    indices_f = np.where(labels)[0] # Get inidices of flagged samples\n","    data_only_corr = data.drop(index = indices_f) # Delete flagged samples \n","    indices_nf = np.random.choice(data_only_corr.index, round(no_samples * size_of_test), replace=False) # Get indicies of correct samples for test\n","    indices_test = np.concatenate([indices_f, indices_nf]) #Get all test indices\n","\n","    X_test = data[data.index.isin(indices_test)]\n","    y_test = labels[labels.index.isin(indices_test)]\n","    X_train = data_only_corr.sample(frac=1).reset_index(drop=True)\n","\n","print(y_test.value_counts())\n","print(X_train.shape)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["0.0    686\n","1.0    167\n","Name: label, dtype: int64\n","(3262, 128)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"du_wm18jmlb3","colab_type":"text"},"source":["##Define Module Architecture\n","\n","Encoder and Decoder separatly just in case."]},{"cell_type":"code","metadata":{"id":"ebzoW5k3nny0","colab_type":"code","colab":{}},"source":["class Encoder(nn.Module):\n","    def __init__(self, no_features, lat_r, dropout):\n","        super().__init__()\n","        NF = no_features\n","        self.fc1 = nn.Linear(NF, round(lat_r[0]*NF))\n","        self.fc2 = nn.Linear(round(lat_r[0]*NF), round(lat_r[1]*NF))\n","        self.fc3 = nn.Linear(round(lat_r[1]*NF), round(lat_r[2]*NF))\n","        self.dropout = nn.Dropout(p=dropout)\n","        \n","    def forward(self, x):\n","        x = self.dropout(F.relu(self.fc1(x)))\n","        x = self.dropout(F.relu(self.fc2(x)))\n","        x = self.dropout(self.fc3(x)) #Only droput on last layer no ReLu\n","        return x\n","\n","class Decoder(nn.Module):\n","    def __init__(self, no_features, lat_r, dropout):\n","        super().__init__()\n","        NF = no_features\n","        self.fc1 = nn.Linear(round(lat_r[2]*NF), round(lat_r[1]*NF))\n","        self.fc2 = nn.Linear(round(lat_r[1]*NF), round(lat_r[0]*NF))\n","        self.fc3 = nn.Linear(round(lat_r[0]*NF), NF)\n","        self.dropout = nn.Dropout(p=dropout)       \n","        \n","    def forward(self, x):\n","        x = self.dropout(F.relu(self.fc1(x)))\n","        x = self.dropout(F.relu(self.fc2(x)))\n","        x = t.sigmoid(self.fc3(x))\n","        return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pyWIVJj_vGPV","colab_type":"code","colab":{}},"source":["class SimpleAutoEncoder(nn.Module):\n","    def __init__(self, no_features, lat_r, dropout):\n","        super().__init__()\n","\n","        self.encoder = Encoder(no_features, lat_r, dropout)\n","        self.decoder = Decoder(no_features, lat_r, dropout)\n","        \n","    def forward(self, X):\n","        encoded = self.encoder(X)\n","        decoded = self.decoder(encoded)\n","        return decoded"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ssClUk3AI7pF","colab_type":"text"},"source":["##Check GPU aviability"]},{"cell_type":"code","metadata":{"id":"f1vsrKGw8mDV","colab_type":"code","outputId":"fdea89bc-c5e1-4d82-9b6d-b4197016845c","executionInfo":{"status":"ok","timestamp":1574347985741,"user_tz":-60,"elapsed":16980,"user":{"displayName":"Bart≈Çomiej Cerek","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDlsR1yZSDN4MjBT0j3iDjqMfyCHSbnh0KNKZ9Q5rA=s64","userId":"03865172822680659937"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# check if CUDA is available\n","gpu = t.cuda.is_available()\n","if not gpu:\n","    print('CUDA is not available.  Training on CPU ...')\n","else:\n","    print('CUDA is available!  Training on GPU ...')"],"execution_count":10,"outputs":[{"output_type":"stream","text":["CUDA is available!  Training on GPU ...\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"z0FORJwOILsK","colab_type":"text"},"source":["##Set parameters and define model"]},{"cell_type":"code","metadata":{"id":"ix083xRSDgKC","colab_type":"code","colab":{}},"source":["#Parameters setting\n","max_epochs = 70\n","dropout = 0.2\n","batch_size = 32\n","start_lr = 0.007 #Starting learning rate \n","\n","#Learning Rate scheduler\n","patience = 5 #After how many epochs with no improvemnt change LR\n","factor = 0.8 #Scale factor of LR \n","\n","#Encoding\n","latent_ratio = [0.5, 0.3, 0.2] #Must be 3 elements"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GfROpQRz0aGc","colab_type":"code","colab":{}},"source":["#Create callbacks\n","model_path = 'skorch_ae_model.pt'\n","checkpoint = Checkpoint(f_params=model_path, monitor='valid_loss_best')\n","scheduler = LRScheduler(policy='ReduceLROnPlateau', monitor='valid_loss', factor = factor, patience = patience, threshold=1e-3) \n","\n","net = NeuralNetRegressor(\n","    SimpleAutoEncoder,\n","    module__no_features = no_features,\n","    module__lat_r = latent_ratio,\n","    module__dropout = dropout,\n","    batch_size = batch_size,\n","    lr=start_lr,\n","    optimizer = optim.Adam,\n","    max_epochs=max_epochs,\n","    train_split=skorch.dataset.CVSplit(3),\n","    device='cuda' if gpu else 'cpu',\n","    callbacks=[checkpoint, scheduler],\n","    verbose = 1\n",")\n","#Default criterion is <class 'torch.nn.modules.loss.MSELoss'>"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z-x2qgMm24iu","colab_type":"text"},"source":["##Train Autoencoder"]},{"cell_type":"code","metadata":{"id":"LBlg-atY28GW","colab_type":"code","outputId":"4f660d7e-be69-43eb-fa53-c90946d2f6bd","executionInfo":{"status":"ok","timestamp":1574348012185,"user_tz":-60,"elapsed":43410,"user":{"displayName":"Bart≈Çomiej Cerek","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDlsR1yZSDN4MjBT0j3iDjqMfyCHSbnh0KNKZ9Q5rA=s64","userId":"03865172822680659937"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["X = t.FloatTensor(X_train.to_numpy())\n","_ = net.fit(X,X) #output compared with imput, hide output"],"execution_count":13,"outputs":[{"output_type":"stream","text":["  epoch    train_loss    valid_loss    cp     dur\n","-------  ------------  ------------  ----  ------\n","      1        \u001b[36m0.0179\u001b[0m        \u001b[32m0.0068\u001b[0m     +  0.3318\n","      2        \u001b[36m0.0073\u001b[0m        \u001b[32m0.0068\u001b[0m     +  0.2556\n","      3        \u001b[36m0.0071\u001b[0m        \u001b[32m0.0067\u001b[0m     +  0.2443\n","      4        \u001b[36m0.0070\u001b[0m        \u001b[32m0.0067\u001b[0m     +  0.2609\n","      5        \u001b[36m0.0063\u001b[0m        \u001b[32m0.0044\u001b[0m     +  0.2518\n","      6        \u001b[36m0.0048\u001b[0m        \u001b[32m0.0038\u001b[0m     +  0.2599\n","      7        \u001b[36m0.0043\u001b[0m        \u001b[32m0.0035\u001b[0m     +  0.2607\n","      8        \u001b[36m0.0041\u001b[0m        \u001b[32m0.0034\u001b[0m     +  0.2768\n","      9        0.0043        \u001b[32m0.0034\u001b[0m     +  0.2685\n","     10        \u001b[36m0.0040\u001b[0m        \u001b[32m0.0032\u001b[0m     +  0.2697\n","     11        \u001b[36m0.0040\u001b[0m        0.0033        0.2864\n","     12        \u001b[36m0.0038\u001b[0m        \u001b[32m0.0032\u001b[0m     +  0.2571\n","     13        \u001b[36m0.0038\u001b[0m        0.0033        0.2595\n","     14        \u001b[36m0.0036\u001b[0m        \u001b[32m0.0029\u001b[0m     +  0.2533\n","     15        \u001b[36m0.0035\u001b[0m        \u001b[32m0.0029\u001b[0m     +  0.2513\n","     16        \u001b[36m0.0034\u001b[0m        \u001b[32m0.0027\u001b[0m     +  0.2666\n","     17        0.0035        0.0029        0.2671\n","     18        \u001b[36m0.0033\u001b[0m        0.0028        0.2546\n","     19        0.0034        0.0028        0.2597\n","     20        0.0036        0.0029        0.2756\n","     21        0.0035        0.0028        0.2582\n","     22        0.0033        0.0028        0.2636\n","     23        \u001b[36m0.0032\u001b[0m        0.0028        0.2556\n","     24        \u001b[36m0.0032\u001b[0m        \u001b[32m0.0027\u001b[0m     +  0.2687\n","     25        \u001b[36m0.0031\u001b[0m        \u001b[32m0.0027\u001b[0m     +  0.2597\n","     26        0.0031        \u001b[32m0.0026\u001b[0m     +  0.2625\n","     27        \u001b[36m0.0030\u001b[0m        \u001b[32m0.0026\u001b[0m     +  0.2725\n","     28        0.0031        0.0027        0.2663\n","     29        0.0031        0.0026        0.2573\n","     30        0.0030        0.0026        0.2535\n","     31        0.0031        \u001b[32m0.0026\u001b[0m     +  0.2709\n","     32        \u001b[36m0.0030\u001b[0m        \u001b[32m0.0025\u001b[0m     +  0.2547\n","     33        0.0031        \u001b[32m0.0025\u001b[0m     +  0.2510\n","     34        0.0030        0.0026        0.2806\n","     35        0.0031        \u001b[32m0.0025\u001b[0m     +  0.2663\n","     36        0.0030        0.0027        0.2846\n","     37        0.0030        0.0027        0.3335\n","     38        0.0030        \u001b[32m0.0025\u001b[0m     +  0.2636\n","     39        0.0031        0.0026        0.2605\n","     40        \u001b[36m0.0029\u001b[0m        \u001b[32m0.0025\u001b[0m     +  0.2550\n","     41        0.0030        0.0026        0.2780\n","     42        0.0030        0.0026        0.2676\n","     43        0.0029        \u001b[32m0.0025\u001b[0m     +  0.2577\n","     44        \u001b[36m0.0028\u001b[0m        0.0025        0.2682\n","     45        0.0029        0.0026        0.2886\n","     46        0.0032        0.0027        0.2567\n","     47        0.0031        0.0027        0.2572\n","     48        0.0031        0.0026        0.2775\n","     49        0.0030        0.0025        0.2586\n","     50        0.0029        0.0027        0.2499\n","     51        0.0029        0.0025        0.2525\n","     52        0.0029        0.0025        0.2772\n","     53        \u001b[36m0.0028\u001b[0m        0.0026        0.2504\n","     54        0.0029        0.0025        0.2606\n","     55        0.0029        0.0027        0.2633\n","     56        0.0028        \u001b[32m0.0024\u001b[0m     +  0.2616\n","     57        \u001b[36m0.0027\u001b[0m        0.0025        0.2494\n","     58        0.0027        \u001b[32m0.0024\u001b[0m     +  0.2697\n","     59        0.0028        0.0027        0.2942\n","     60        0.0028        0.0025        0.2686\n","     61        0.0027        0.0024        0.2688\n","     62        \u001b[36m0.0027\u001b[0m        0.0024        0.2911\n","     63        \u001b[36m0.0027\u001b[0m        0.0024        0.2617\n","     64        \u001b[36m0.0026\u001b[0m        \u001b[32m0.0023\u001b[0m     +  0.2506\n","     65        0.0027        0.0024        0.2621\n","     66        0.0027        0.0023        0.2607\n","     67        0.0026        \u001b[32m0.0023\u001b[0m     +  0.2561\n","     68        0.0026        0.0026        0.2639\n","     69        0.0027        0.0024        0.3212\n","     70        \u001b[36m0.0026\u001b[0m        0.0025        0.2605\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"uhnctkCS4q8v","colab_type":"text"},"source":["##Test model"]},{"cell_type":"code","metadata":{"id":"CMb-jMSNV_fz","colab_type":"code","colab":{}},"source":["#Define function calculating averge losses\n","def aeTestLosses(X_test, X_pred, y_test):\n","    '''\n","    X_test, X_pred need to be tensors!\n","    Return average MSE of OUTLIERS and CORRECT samples in this order.\n","    '''\n","    outliers = []\n","    correct = []\n","    loss_func = nn.MSELoss()\n","    for t,p,l in zip(X_test, X_pred, y_test):\n","        if l: #If outlier\n","            outliers.append(loss_func(t,p))\n","        else:\n","            correct.append(loss_func(t,p))\n","    #Return calculated averages as normal numbers        \n","    return (sum(outliers)/len(outliers)).item() , (sum(correct)/len(correct)).item()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MY0MqaMbfCqg","colab_type":"text"},"source":["**Load best model from checkpoint**"]},{"cell_type":"code","metadata":{"id":"ki_ccXh8ecN7","colab_type":"code","outputId":"c308d474-88cb-43e2-aad6-b72b4035b8da","executionInfo":{"status":"ok","timestamp":1574348012187,"user_tz":-60,"elapsed":43399,"user":{"displayName":"Bart≈Çomiej Cerek","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDlsR1yZSDN4MjBT0j3iDjqMfyCHSbnh0KNKZ9Q5rA=s64","userId":"03865172822680659937"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["#Load model with lowest validation accuracy\n","net.initialize()  # This is important!\n","net.load_params(model_path)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Re-initializing module because the following parameters were re-set: dropout, lat_r, no_features.\n","Re-initializing optimizer.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"EOKHCwvxCXuu","colab_type":"text"},"source":["**Run prediction and test** "]},{"cell_type":"code","metadata":{"id":"CUXYj1CbYI_w","colab_type":"code","outputId":"b9b52461-91d5-435a-8884-1aaac1ffa190","executionInfo":{"status":"ok","timestamp":1574348012752,"user_tz":-60,"elapsed":43957,"user":{"displayName":"Bart≈Çomiej Cerek","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDlsR1yZSDN4MjBT0j3iDjqMfyCHSbnh0KNKZ9Q5rA=s64","userId":"03865172822680659937"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["X_test_t = t.FloatTensor(X_test.to_numpy()) #Test data to tensor\n","pred = net.forward(X_test_t)\n","o,c = aeTestLosses(X_test_t, pred, y_test)\n","\n","print('Score of AE: {:0.2f}mu'.format((o-c)*1000))"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Score of AE: 53.90mu\n"],"name":"stdout"}]}]}